# MÃ©tricas de EvaluaciÃ³n para Clasificadores

## 1. Â¿Por quÃ© Accuracy No Es Suficiente?

### El Problema del Desbalance de Clases

```
Escenario: Detector de fraude bancario

Dataset: 10,000 transacciones
  â€¢ 9,900 legÃ­timas (99%)
  â€¢ 100 fraudulentas (1%)

Modelo "tonto": Predice SIEMPRE "legÃ­timo"
  Accuracy = 9,900 / 10,000 = 99% ğŸ‰

Â¿99% accuracy es bueno?
  â†’ NO detecta NINGÃšN fraude
  â†’ Es completamente inÃºtil
  â†’ El accuracy miente
```

### Necesitamos MÃ©tricas MÃ¡s Informativas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MÃ‰TRICAS DE CLASIFICACIÃ“N                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  â€¢ Accuracy: % de predicciones correctas (limitada)   â”‚
â”‚  â€¢ Precision: Â¿CuÃ¡ntos positivos predichos son reales?â”‚
â”‚  â€¢ Recall: Â¿CuÃ¡ntos positivos reales detectamos?      â”‚
â”‚  â€¢ F1-Score: Balance entre Precision y Recall         â”‚
â”‚  â€¢ Matriz de ConfusiÃ³n: Desglose completo de errores  â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. Matriz de ConfusiÃ³n

### Estructura

```
                        PREDICCIÃ“N
                    â”‚  Negativo  â”‚  Positivo
          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REAL      Negativo  â”‚    TN      â”‚    FP
          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          Positivo  â”‚    FN      â”‚    TP


TN = True Negative   (Correctamente rechazado)
FP = False Positive  (Falsa alarma)
FN = False Negative  (Fallo de detecciÃ³n)
TP = True Positive   (Correctamente detectado)
```

### Ejemplo: Detector de SPAM

```
                          PREDICCIÃ“N
                      â”‚    HAM    â”‚   SPAM
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 REAL       HAM       â”‚    960    â”‚     6
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            SPAM      â”‚     29    â”‚   120


InterpretaciÃ³n:
  TN = 960: Emails HAM correctamente clasificados como HAM
  FP = 6:   Emails HAM incorrectamente clasificados como SPAM
  FN = 29:  Emails SPAM que se escaparon (clasificados como HAM)
  TP = 120: Emails SPAM correctamente detectados
```

### VisualizaciÃ³n GrÃ¡fica

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PREDICCIÃ“N                           â”‚
â”‚              HAM                SPAM                    â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚    HAM  â”‚     TN       â”‚     FP       â”‚                â”‚
â”‚         â”‚    960       â”‚      6       â”‚  â† OK (pocos)  â”‚
â”‚ R       â”‚  âœ“ Correcto  â”‚  âœ— Falsa     â”‚                â”‚
â”‚ E       â”‚              â”‚    alarma    â”‚                â”‚
â”‚ A       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
â”‚ L       â”‚     FN       â”‚     TP       â”‚                â”‚
â”‚    SPAM â”‚     29       â”‚    120       â”‚                â”‚
â”‚         â”‚  âœ— PELIGRO   â”‚  âœ“ Correcto  â”‚                â”‚
â”‚         â”‚   Se escapÃ³  â”‚              â”‚                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                         â”‚
â”‚  FN (False Negative) es el mÃ¡s peligroso en seguridad: â”‚
â”‚  SPAM/Malware/Ataque que NO detectamos                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3. Accuracy

### FÃ³rmula

```
              TP + TN
Accuracy = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           TP + TN + FP + FN


              Predicciones correctas
         = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             Total de predicciones
```

### CÃ¡lculo con Ejemplo

```
             120 + 960        1080
Accuracy = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = 0.9686 (96.86%)
           120+960+6+29       1115

InterpretaciÃ³n:
  El 96.86% de las predicciones son correctas.
  Pero esto NO nos dice cÃ³mo se distribuyen los errores.
```

## 4. Precision (PrecisiÃ³n)

### DefiniciÃ³n

**Precision:** De todos los que predije como POSITIVO, Â¿cuÃ¡ntos realmente lo son?

```
                  TP
Precision = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              TP + FP

            Verdaderos Positivos
          = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            Total predichos Positivos
```

### CÃ¡lculo e InterpretaciÃ³n

```
                120
Precision = â”€â”€â”€â”€â”€â”€â”€â”€â”€ = 0.952 (95.2%)
             120 + 6

InterpretaciÃ³n:
  De todos los emails que el modelo marcÃ³ como SPAM,
  el 95.2% realmente ERAN spam.

  Solo el 4.8% fueron falsas alarmas (emails legÃ­timos
  marcados como SPAM).
```

### CuÃ¡ndo Importa Precision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRECISION ES CRÃTICA CUANDO:                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  â€¢ Falsos Positivos son COSTOSOS                       â”‚
â”‚                                                        â”‚
â”‚  Ejemplos:                                             â”‚
â”‚    â€¢ Email legÃ­timo â†’ carpeta SPAM (usuario molesto)   â”‚
â”‚    â€¢ Usuario legÃ­timo â†’ bloqueado (pÃ©rdida de cliente) â”‚
â”‚    â€¢ TransacciÃ³n legal â†’ rechazada (pÃ©rdida de venta) â”‚
â”‚                                                        â”‚
â”‚  Alta Precision = Pocas falsas alarmas                â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 5. Recall (Sensibilidad / Exhaustividad)

### DefiniciÃ³n

**Recall:** De todos los POSITIVOS reales, Â¿cuÃ¡ntos detectÃ©?

```
               TP
Recall = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           TP + FN

         Verdaderos Positivos
       = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Total realmente Positivos
```

### CÃ¡lculo e InterpretaciÃ³n

```
              120
Recall = â”€â”€â”€â”€â”€â”€â”€â”€â”€ = 0.805 (80.5%)
          120 + 29

InterpretaciÃ³n:
  De todos los emails que REALMENTE eran SPAM,
  el modelo detectÃ³ el 80.5%.

  El 19.5% de los SPAM se escaparon y llegaron
  a la bandeja de entrada.
```

### CuÃ¡ndo Importa Recall

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RECALL ES CRÃTICO CUANDO:                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  â€¢ Falsos Negativos son PELIGROSOS                     â”‚
â”‚                                                        â”‚
â”‚  Ejemplos:                                             â”‚
â”‚    â€¢ Malware â†’ clasificado como benigno (infecciÃ³n)   â”‚
â”‚    â€¢ Ataque â†’ no detectado (brecha de seguridad)      â”‚
â”‚    â€¢ CÃ¡ncer â†’ no diagnosticado (riesgo vital)         â”‚
â”‚    â€¢ Fraude â†’ no detectado (pÃ©rdida econÃ³mica)        â”‚
â”‚                                                        â”‚
â”‚  Alto Recall = Pocos casos peligrosos sin detectar    â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 6. El Trade-off Precision vs Recall

### No Puedes Maximizar Ambos

```
         Precision
            â”‚
        1.0 â”‚â—
            â”‚ â•²
        0.8 â”‚  â•²
            â”‚   â•²         La curva muestra que
        0.6 â”‚    â•²        al aumentar uno,
            â”‚     â•²       el otro disminuye
        0.4 â”‚      â•²
            â”‚       â•²
        0.2 â”‚        â•²
            â”‚         â—
        0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Recall
            0  0.2 0.4 0.6 0.8 1.0


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRADE-OFF                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Umbral ALTO (ej: 0.9):                                â”‚
â”‚    â€¢ Solo predigo SPAM si estoy MUY seguro             â”‚
â”‚    â€¢ Precision ALTA (pocos falsos positivos)           â”‚
â”‚    â€¢ Recall BAJO (muchos SPAM se escapan)              â”‚
â”‚                                                         â”‚
â”‚  Umbral BAJO (ej: 0.3):                                â”‚
â”‚    â€¢ Predigo SPAM ante cualquier sospecha              â”‚
â”‚    â€¢ Recall ALTO (capturo casi todo el SPAM)           â”‚
â”‚    â€¢ Precision BAJA (muchas falsas alarmas)            â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ejemplo con Diferentes Umbrales

```
Umbral = 0.9 (muy conservador):
  "Solo marco como SPAM si P(spam) > 0.9"

  Precision: 98%  (casi no hay falsas alarmas)
  Recall:    45%  (mÃ¡s de la mitad del SPAM se escapa)


Umbral = 0.5 (estÃ¡ndar):
  "Marco como SPAM si P(spam) > 0.5"

  Precision: 95%
  Recall:    80%


Umbral = 0.2 (agresivo):
  "Marco como SPAM ante cualquier sospecha"

  Precision: 65%  (muchas falsas alarmas)
  Recall:    98%  (casi no se escapa nada)
```

## 7. F1-Score: El Balance

### DefiniciÃ³n

**F1-Score:** Media armÃ³nica de Precision y Recall.

```
              2 Ã— Precision Ã— Recall
F1-Score = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              Precision + Recall


Â¿Por quÃ© media ARMÃ“NICA y no aritmÃ©tica?
  â†’ Penaliza mÃ¡s cuando uno de los dos es muy bajo
  â†’ Solo es alto si AMBOS son altos
```

### ComparaciÃ³n de Medias

```
Escenario: Precision = 0.95, Recall = 0.10

Media aritmÃ©tica: (0.95 + 0.10) / 2 = 0.525
  â†’ Parece "decente" pero el Recall es terrible

Media armÃ³nica (F1): 2Ã—0.95Ã—0.10 / (0.95+0.10) = 0.181
  â†’ Refleja que el modelo es malo en Recall
```

### CÃ¡lculo con Ejemplo

```
Precision = 0.952
Recall = 0.805

           2 Ã— 0.952 Ã— 0.805
F1-Score = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = 0.872 (87.2%)
            0.952 + 0.805

InterpretaciÃ³n:
  F1 = 87.2% indica un buen balance entre
  detectar SPAM (Recall) y no generar
  falsas alarmas (Precision).
```

### Variantes de F-Score

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  F-BETA SCORE                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  F_Î² = (1 + Î²Â²) Ã— (Precision Ã— Recall)                â”‚
â”‚        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚        (Î²Â² Ã— Precision) + Recall                       â”‚
â”‚                                                        â”‚
â”‚  Î² = 1:   F1 (balance igual)                          â”‚
â”‚  Î² = 0.5: F0.5 (prioriza Precision)                   â”‚
â”‚  Î² = 2:   F2 (prioriza Recall)                        â”‚
â”‚                                                        â”‚
â”‚  En SEGURIDAD: F2 suele ser mejor                     â”‚
â”‚  (preferimos detectar todo aunque haya falsas alarmas)â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 8. Classification Report Completo

### Formato EstÃ¡ndar

```
              precision    recall  f1-score   support

         ham       0.97      1.00      0.98       966
        spam       0.95      0.81      0.87       149

    accuracy                           0.97      1115
   macro avg       0.96      0.90      0.93      1115
weighted avg       0.97      0.97      0.97      1115
```

### InterpretaciÃ³n LÃ­nea por LÃ­nea

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DESGLOSE DEL CLASSIFICATION REPORT                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  ham:                                                   â”‚
â”‚    precision=0.97: 97% de los predichos HAM son HAM    â”‚
â”‚    recall=1.00: Detectamos 100% de los HAM reales      â”‚
â”‚    f1-score=0.98: Excelente balance para HAM           â”‚
â”‚    support=966: HabÃ­a 966 emails HAM en test           â”‚
â”‚                                                         â”‚
â”‚  spam:                                                  â”‚
â”‚    precision=0.95: 95% de los predichos SPAM son SPAM  â”‚
â”‚    recall=0.81: Solo detectamos 81% del SPAM real      â”‚
â”‚    f1-score=0.87: Buen balance pero Recall mejorable   â”‚
â”‚    support=149: HabÃ­a 149 emails SPAM en test          â”‚
â”‚                                                         â”‚
â”‚  macro avg: Promedio simple de cada mÃ©trica            â”‚
â”‚  weighted avg: Promedio ponderado por support          â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 9. CuÃ¡ndo Usar Cada MÃ©trica

### GuÃ­a de DecisiÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MÃ‰TRICA      â”‚           USAR CUANDO                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚                                      â”‚
â”‚   ACCURACY      â”‚  â€¢ Clases balanceadas               â”‚
â”‚                 â”‚  â€¢ Todos los errores igual de malos â”‚
â”‚                 â”‚                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚                                      â”‚
â”‚   PRECISION     â”‚  â€¢ FP son costosos                  â”‚
â”‚                 â”‚  â€¢ Bloquear usuario legÃ­timo = malo â”‚
â”‚                 â”‚  â€¢ Email importante â†’ SPAM = malo   â”‚
â”‚                 â”‚                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚                                      â”‚
â”‚   RECALL        â”‚  â€¢ FN son peligrosos                â”‚
â”‚                 â”‚  â€¢ Malware no detectado = desastre  â”‚
â”‚                 â”‚  â€¢ Ataque sin alertar = crÃ­tico     â”‚
â”‚                 â”‚                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚                                      â”‚
â”‚   F1-SCORE      â”‚  â€¢ Necesitas balance                â”‚
â”‚                 â”‚  â€¢ Clases desbalanceadas            â”‚
â”‚                 â”‚  â€¢ Comparar modelos                 â”‚
â”‚                 â”‚                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚                                      â”‚
â”‚   F2-SCORE      â”‚  â€¢ Recall mÃ¡s importante            â”‚
â”‚                 â”‚  â€¢ Seguridad, medicina              â”‚
â”‚                 â”‚  â€¢ Mejor sobre-alertar que perder   â”‚
â”‚                 â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ejemplos por Dominio

```
DETECTOR DE MALWARE:
  Prioridad: Recall (F2)
  RazÃ³n: Un malware no detectado puede infectar la red
  Falso positivo = anÃ¡lisis extra (molesto pero seguro)
  Falso negativo = infecciÃ³n (desastre)


FILTRO DE SPAM:
  Prioridad: Balance (F1) o ligera prioridad a Precision
  RazÃ³n: Email importante en SPAM = usuario enfadado
  Mejor dejar pasar algo de SPAM que perder emails


DETECTOR DE FRAUDE:
  Prioridad: Recall (F2)
  RazÃ³n: Fraude no detectado = pÃ©rdida econÃ³mica
  TransacciÃ³n legÃ­tima bloqueada = llamada al banco


SISTEMA DE ALERTAS DE SEGURIDAD:
  Prioridad: Recall, pero vigilar Precision
  RazÃ³n: Muchas falsas alarmas = "fatiga de alertas"
  El equipo ignora alertas si son siempre falsas
```

## 10. CÃ³digo Python

### Calcular Todas las MÃ©tricas

```python
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    classification_report,
    confusion_matrix
)

# DespuÃ©s de hacer predicciones
y_pred = model.predict(X_test)

# MÃ©tricas individuales
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-Score:  {f1:.4f}")

# Matriz de confusiÃ³n
cm = confusion_matrix(y_test, y_pred)
print(f"\nMatriz de ConfusiÃ³n:")
print(cm)

# Reporte completo
print(classification_report(y_test, y_pred,
                           target_names=['ham', 'spam']))
```

### Visualizar Matriz de ConfusiÃ³n

```python
import matplotlib.pyplot as plt
import seaborn as sns

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['HAM', 'SPAM'],
            yticklabels=['HAM', 'SPAM'])
plt.xlabel('PredicciÃ³n')
plt.ylabel('Real')
plt.title('Matriz de ConfusiÃ³n')
plt.show()
```

## 11. Resumen

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MÃ‰TRICAS DE CLASIFICACIÃ“N - RESUMEN                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  MATRIZ DE CONFUSIÃ“N:                                         â”‚
â”‚    TP, TN, FP, FN - Desglose de predicciones                 â”‚
â”‚                                                               â”‚
â”‚  ACCURACY = (TP + TN) / Total                                â”‚
â”‚    Limitada en clases desbalanceadas                         â”‚
â”‚                                                               â”‚
â”‚  PRECISION = TP / (TP + FP)                                  â”‚
â”‚    "De lo que predije positivo, Â¿cuÃ¡nto es real?"            â”‚
â”‚    Alta cuando importa evitar falsas alarmas                 â”‚
â”‚                                                               â”‚
â”‚  RECALL = TP / (TP + FN)                                     â”‚
â”‚    "De lo positivo real, Â¿cuÃ¡nto detectÃ©?"                   â”‚
â”‚    Alta cuando importa no perder casos                       â”‚
â”‚                                                               â”‚
â”‚  F1 = 2Ã—PÃ—R / (P+R)                                          â”‚
â”‚    Balance entre Precision y Recall                          â”‚
â”‚    Usar cuando ambos importan                                â”‚
â”‚                                                               â”‚
â”‚  TRADE-OFF:                                                   â”‚
â”‚    Subir umbral â†’ mÃ¡s Precision, menos Recall                â”‚
â”‚    Bajar umbral â†’ mÃ¡s Recall, menos Precision                â”‚
â”‚                                                               â”‚
â”‚  EN CIBERSEGURIDAD:                                           â”‚
â”‚    Generalmente priorizar RECALL (no perder ataques)         â”‚
â”‚    Pero vigilar Precision (evitar fatiga de alertas)         â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Siguiente:** Persistencia de modelos y MLOps bÃ¡sico
