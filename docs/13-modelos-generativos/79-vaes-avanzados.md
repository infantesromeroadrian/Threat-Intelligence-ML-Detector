# Variational Autoencoders Avanzados

## Introduccion

Los **Variational Autoencoders (VAEs)** son modelos generativos que aprenden representaciones latentes de datos de forma probabilistica. A diferencia de los autoencoders clasicos, los VAEs modelan la distribucion latente explicitamente, permitiendo generacion de nuevas muestras.

```
VAE vs AUTOENCODER CLASICO
==========================

Autoencoder Clasico:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    x â”€â”€â”€â–¶ Encoder â”€â”€â”€â–¶ z â”€â”€â”€â–¶ Decoder â”€â”€â”€â–¶ xÌ‚

    - z es un vector DETERMINISTA
    - Solo reconstruccion, no generacion
    - Espacio latente puede ser discontinuo


VAE (Variational Autoencoder):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                          â”Œâ”€â”€â”€â”€â”€â”
    x â”€â”€â”€â–¶ Encoder â”€â”€â”€â”¬â”€â”€â”€â”‚  Î¼  â”‚â”€â”€â”
                      â”‚   â””â”€â”€â”€â”€â”€â”˜  â”‚
                      â”‚            â”œâ”€â”€â”€â–¶ z = Î¼ + ÏƒâŠ™Îµ â”€â”€â”€â–¶ Decoder â”€â”€â”€â–¶ xÌ‚
                      â”‚   â”Œâ”€â”€â”€â”€â”€â”  â”‚
                      â””â”€â”€â”€â”‚  Ïƒ  â”‚â”€â”€â”˜
                          â””â”€â”€â”€â”€â”€â”˜
                              â–²
                              â”‚
                         Îµ ~ N(0,I)

    - z es una variable ALEATORIA
    - Puede generar nuevas muestras
    - Espacio latente continuo y estructurado


DISTRIBUCION LATENTE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                 Autoencoder              VAE
                 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”€â”€â”€

Espacio             â—                     â•­â”€â”€â”€â”€â”€â•®
latente           â—   â—                  â”‚     â”‚
                 â—  â—  â—                 â”‚ N(Î¼,Ïƒ)â”‚
                   â— â—                   â”‚     â”‚
                                          â•°â”€â”€â”€â”€â”€â•¯

              Puntos discretos        Distribucion continua
              (sin estructura)        (muestreo posible)
```

---

## Matematicas del VAE: ELBO

### Objetivo: Maximizar la Verosimilitud

```
DERIVACION DEL ELBO
===================

Queremos maximizar log p(x), la log-verosimilitud de los datos.

Problema: p(x) = âˆ« p(x|z)p(z) dz  es INTRATABLE

Solucion: Usar una distribucion aproximada q(z|x) â‰ˆ p(z|x)


PASO 1: Identidad basica
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

log p(x) = log p(x) Â· âˆ« q(z|x) dz      (integrando 1)
         = âˆ« q(z|x) log p(x) dz
         = âˆ« q(z|x) log [p(x,z)/p(z|x)] dz

         = âˆ« q(z|x) log [p(x,z) Â· q(z|x) / (p(z|x) Â· q(z|x))] dz

         = âˆ« q(z|x) log [p(x,z)/q(z|x)] dz + âˆ« q(z|x) log [q(z|x)/p(z|x)] dz

         = E_q[log p(x,z) - log q(z|x)] + KL(q(z|x) || p(z|x))

         = ELBO + KL(q||p)


PASO 2: Reordenando
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

log p(x) = ELBO + KL(q(z|x) || p(z|x))

Como KL â‰¥ 0:

    log p(x) â‰¥ ELBO

El ELBO es un LIMITE INFERIOR de log p(x).
Maximizar ELBO â‰ˆ Maximizar log p(x)


PASO 3: Descomposicion del ELBO
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ELBO = E_q[log p(x,z) - log q(z|x)]

     = E_q[log p(x|z) + log p(z) - log q(z|x)]

     = E_q[log p(x|z)] - E_q[log q(z|x) - log p(z)]

     = E_q[log p(x|z)] - KL(q(z|x) || p(z))

       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       Reconstruccion    Regularizacion


INTERPRETACION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                    ELBO
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                       â”‚
          â–¼                       â–¼
    Reconstruccion           Regularizacion
    E_q[log p(x|z)]         -KL(q(z|x)||p(z))
          â”‚                       â”‚
          â”‚                       â”‚
    "La imagen                "El latent z
     reconstruida              debe parecerse
     debe ser igual            a una gaussiana
     a la original"            N(0, I)"
```

### Calculo Explicito del ELBO

```
ELBO PARA GAUSSIANAS
====================

Asumimos:
    - Prior:     p(z) = N(0, I)
    - Encoder:   q(z|x) = N(Î¼(x), ÏƒÂ²(x)I)  donde Î¼, Ïƒ son redes neuronales
    - Decoder:   p(x|z) = N(f(z), Ïƒ_decÂ²I)  donde f es red neuronal


TERMINO DE RECONSTRUCCION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

E_q[log p(x|z)] = E_q[-||x - f(z)||Â² / (2Ïƒ_decÂ²) - const]

                â‰ˆ -||x - f(Î¼ + ÏƒâŠ™Îµ)||Â² / (2Ïƒ_decÂ²)

                â‰ˆ -||x - xÌ‚||Â²  (MSE loss, asumiendo Ïƒ_dec=1)


TERMINO DE KL:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

KL(q(z|x) || p(z)) = KL(N(Î¼, ÏƒÂ²I) || N(0, I))

Para dos gaussianas univariadas:
    KL(N(Î¼, ÏƒÂ²) || N(0, 1)) = -Â½[1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²]

Para D dimensiones:
    KL = -Â½ Î£áµ¢ [1 + log(Ïƒáµ¢Â²) - Î¼áµ¢Â² - Ïƒáµ¢Â²]


LOSS FINAL:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

L(Î¸, Ï†) = -ELBO
        = ||x - xÌ‚||Â² + Î² Â· KL(q||p)

Donde:
    - Î¸: parametros del decoder
    - Ï†: parametros del encoder
    - Î²: peso del termino KL (Î²=1 para VAE estandar)


DIAGRAMA DE LA LOSS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    x â”€â”€â”€â”€â”€â”€â”
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”
    â”‚    Encoder    â”‚â”€â”€â”€â”€â”€â”€â”‚  Î¼  â”‚â”€â”€â”€â”
    â”‚     q_Ï†       â”‚      â””â”€â”€â”€â”€â”€â”˜   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”Œâ”€â”€â”€â”€â”€â”   â”‚
                           â”‚log Ïƒâ”‚â”€â”€â”€â”¤
                           â””â”€â”€â”€â”€â”€â”˜   â”‚
                                     â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                              â”‚ z = Î¼ + ÏƒâŠ™Îµ â”‚
                              â”‚   (sample)  â”‚
                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    xÌ‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    Decoder    â”‚
                           â”‚     p_Î¸       â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


    Loss = MSE(x, xÌ‚) + Î² Â· KL(N(Î¼,ÏƒÂ²) || N(0,1))
           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           Reconstruir     Regularizar latent
```

---

## Reparameterization Trick

```
REPARAMETERIZATION TRICK
========================

Problema: Â¿Como backpropagar a traves de z ~ q(z|x)?

El sampling es una operacion ESTOCASTICA y no diferenciable.


SOLUCION: Reparametrizacion
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

En lugar de:
    z ~ N(Î¼, ÏƒÂ²)

Escribimos:
    Îµ ~ N(0, 1)
    z = Î¼ + Ïƒ âŠ™ Îµ

El gradiente ahora puede fluir:

    âˆ‚L/âˆ‚Î¼ = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚Î¼ = âˆ‚L/âˆ‚z Â· 1
    âˆ‚L/âˆ‚Ïƒ = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚Ïƒ = âˆ‚L/âˆ‚z Â· Îµ


VISUALIZACION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Sin reparametrizacion:              Con reparametrizacion:

     Î¼, ÏƒÂ²                               Î¼, Ïƒ
       â”‚                                  â”‚    Îµ ~ N(0,1)
       â–¼                                  â”‚    â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”                              â”‚    â”‚
   â”‚Sample â”‚â—€â”€â”€â”€â”€ No gradiente!           â”‚    â–¼
   â”‚z~N(Î¼,Ïƒ)â”‚                            z = Î¼ + ÏƒâŠ™Îµ
   â””â”€â”€â”€â”¬â”€â”€â”€â”˜                                  â”‚
       â”‚                                      â”‚
       â–¼                                      â–¼
    Decoder                              Decoder
       â”‚                                      â”‚
       â–¼                                      â–¼
      Loss                                  Loss
       â”‚                                      â”‚
       âœ— No backprop                      âœ“ Backprop OK!


CODIGO:
â”€â”€â”€â”€â”€â”€â”€

# Sin reparametrizacion (NO funciona para training)
z = torch.normal(mu, sigma)  # No tiene gradiente

# Con reparametrizacion (SI funciona)
eps = torch.randn_like(mu)
z = mu + sigma * eps  # Gradientes fluyen a mu y sigma
```

---

## Implementacion Completa de VAE

```python
"""
VAE: Variational Autoencoder
Implementacion completa con PyTorch.
"""

from typing import Tuple, List, Optional
from dataclasses import dataclass
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt


@dataclass
class VAEConfig:
    """Configuracion del VAE."""

    input_dim: int = 784  # MNIST: 28*28
    hidden_dims: Tuple[int, ...] = (512, 256)
    latent_dim: int = 20
    beta: float = 1.0  # Peso del KL
    learning_rate: float = 1e-3


class VAEEncoder(nn.Module):
    """
    Encoder: x -> (Î¼, log ÏƒÂ²)
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dims: Tuple[int, ...],
        latent_dim: int,
    ):
        super().__init__()

        # Build encoder MLP
        layers = []
        in_dim = input_dim

        for h_dim in hidden_dims:
            layers.extend([
                nn.Linear(in_dim, h_dim),
                nn.BatchNorm1d(h_dim),
                nn.LeakyReLU(0.2),
            ])
            in_dim = h_dim

        self.encoder = nn.Sequential(*layers)

        # Output layers for Î¼ and log ÏƒÂ²
        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)
        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)

    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:
        """
        Args:
            x: Input [B, input_dim]

        Returns:
            mu: Media [B, latent_dim]
            logvar: Log varianza [B, latent_dim]
        """
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar


class VAEDecoder(nn.Module):
    """
    Decoder: z -> xÌ‚
    """

    def __init__(
        self,
        latent_dim: int,
        hidden_dims: Tuple[int, ...],
        output_dim: int,
    ):
        super().__init__()

        # Build decoder MLP (reverse of encoder)
        layers = []
        in_dim = latent_dim

        for h_dim in reversed(hidden_dims):
            layers.extend([
                nn.Linear(in_dim, h_dim),
                nn.BatchNorm1d(h_dim),
                nn.LeakyReLU(0.2),
            ])
            in_dim = h_dim

        layers.append(nn.Linear(hidden_dims[0], output_dim))
        layers.append(nn.Sigmoid())  # Output en [0, 1]

        self.decoder = nn.Sequential(*layers)

    def forward(self, z: Tensor) -> Tensor:
        """
        Args:
            z: Latent [B, latent_dim]

        Returns:
            x_recon: Reconstruccion [B, output_dim]
        """
        return self.decoder(z)


class VAE(nn.Module):
    """
    Variational Autoencoder completo.
    """

    def __init__(self, config: VAEConfig):
        super().__init__()

        self.config = config
        self.latent_dim = config.latent_dim
        self.beta = config.beta

        self.encoder = VAEEncoder(
            config.input_dim,
            config.hidden_dims,
            config.latent_dim,
        )

        self.decoder = VAEDecoder(
            config.latent_dim,
            config.hidden_dims,
            config.input_dim,
        )

    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:
        """
        Reparameterization trick: z = Î¼ + Ïƒ âŠ™ Îµ

        Args:
            mu: Media [B, latent_dim]
            logvar: Log varianza [B, latent_dim]

        Returns:
            z: Muestra del latent [B, latent_dim]
        """
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + std * eps

    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
        """
        Forward pass completo.

        Args:
            x: Input [B, input_dim]

        Returns:
            x_recon: Reconstruccion [B, input_dim]
            mu: Media [B, latent_dim]
            logvar: Log varianza [B, latent_dim]
        """
        # Encode
        mu, logvar = self.encoder(x)

        # Sample z
        z = self.reparameterize(mu, logvar)

        # Decode
        x_recon = self.decoder(z)

        return x_recon, mu, logvar

    def loss_function(
        self,
        x: Tensor,
        x_recon: Tensor,
        mu: Tensor,
        logvar: Tensor,
    ) -> Tuple[Tensor, Tensor, Tensor]:
        """
        Calcula la loss del VAE (negativo del ELBO).

        Loss = Recon_Loss + Î² * KL_Loss

        Args:
            x: Input original
            x_recon: Reconstruccion
            mu: Media del encoder
            logvar: Log varianza del encoder

        Returns:
            total_loss: Loss total
            recon_loss: Loss de reconstruccion
            kl_loss: Divergencia KL
        """
        # Reconstruction loss (BCE para imagenes binarias)
        recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum')

        # KL divergence: -0.5 * sum(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)
        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

        # Total loss
        total_loss = recon_loss + self.beta * kl_loss

        return total_loss, recon_loss, kl_loss

    @torch.no_grad()
    def sample(self, num_samples: int, device: str = "cuda") -> Tensor:
        """
        Genera nuevas muestras desde el prior p(z) = N(0, I).

        Args:
            num_samples: Numero de muestras
            device: Dispositivo

        Returns:
            Muestras generadas [num_samples, input_dim]
        """
        z = torch.randn(num_samples, self.latent_dim, device=device)
        samples = self.decoder(z)
        return samples

    @torch.no_grad()
    def reconstruct(self, x: Tensor) -> Tensor:
        """
        Reconstruye inputs (encode + decode).
        """
        mu, _ = self.encoder(x)
        x_recon = self.decoder(mu)
        return x_recon

    @torch.no_grad()
    def interpolate(
        self,
        x1: Tensor,
        x2: Tensor,
        num_steps: int = 10,
    ) -> Tensor:
        """
        Interpola entre dos puntos en el espacio latente.

        Args:
            x1, x2: Imagenes a interpolar [1, input_dim]
            num_steps: Numero de pasos de interpolacion

        Returns:
            Interpolaciones [num_steps, input_dim]
        """
        mu1, _ = self.encoder(x1)
        mu2, _ = self.encoder(x2)

        # Interpolacion lineal en espacio latente
        alphas = torch.linspace(0, 1, num_steps, device=x1.device)
        z_interp = torch.stack([
            mu1 * (1 - alpha) + mu2 * alpha
            for alpha in alphas
        ])

        return self.decoder(z_interp.squeeze(1))


def train_vae(
    model: VAE,
    train_loader: DataLoader,
    config: VAEConfig,
    num_epochs: int = 50,
    device: str = "cuda",
) -> List[float]:
    """
    Entrena el VAE.

    Args:
        model: Modelo VAE
        train_loader: DataLoader
        config: Configuracion
        num_epochs: Epocas
        device: Dispositivo

    Returns:
        Lista de losses por epoca
    """
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)

    losses = []

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        total_recon = 0
        total_kl = 0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")

        for batch in pbar:
            # Obtener datos
            if isinstance(batch, (list, tuple)):
                x = batch[0]
            else:
                x = batch

            x = x.view(x.size(0), -1).to(device)

            # Forward
            x_recon, mu, logvar = model(x)

            # Loss
            loss, recon_loss, kl_loss = model.loss_function(x, x_recon, mu, logvar)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Tracking
            total_loss += loss.item()
            total_recon += recon_loss.item()
            total_kl += kl_loss.item()

            pbar.set_postfix({
                "loss": f"{loss.item()/len(x):.2f}",
                "recon": f"{recon_loss.item()/len(x):.2f}",
                "kl": f"{kl_loss.item()/len(x):.2f}",
            })

        avg_loss = total_loss / len(train_loader.dataset)
        losses.append(avg_loss)

        print(f"Epoch {epoch+1}: Loss={avg_loss:.4f}, "
              f"Recon={total_recon/len(train_loader.dataset):.4f}, "
              f"KL={total_kl/len(train_loader.dataset):.4f}")

    return losses


def visualize_latent_space(
    model: VAE,
    test_loader: DataLoader,
    device: str = "cuda",
):
    """Visualiza el espacio latente 2D (solo si latent_dim=2)."""

    if model.latent_dim != 2:
        print("Visualizacion solo disponible para latent_dim=2")
        return

    model.eval()
    zs = []
    labels = []

    with torch.no_grad():
        for batch in test_loader:
            x, y = batch
            x = x.view(x.size(0), -1).to(device)
            mu, _ = model.encoder(x)
            zs.append(mu.cpu())
            labels.append(y)

    zs = torch.cat(zs, dim=0).numpy()
    labels = torch.cat(labels, dim=0).numpy()

    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(zs[:, 0], zs[:, 1], c=labels, cmap='tab10', alpha=0.5, s=5)
    plt.colorbar(scatter)
    plt.xlabel("zâ‚")
    plt.ylabel("zâ‚‚")
    plt.title("VAE Latent Space")
    plt.savefig("vae_latent_space.png", dpi=150)
    plt.show()


# =============================================================================
# EJEMPLO CON MNIST
# =============================================================================

def demo_vae_mnist():
    """Demo de VAE con MNIST."""
    from torchvision import datasets, transforms

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Dataset
    transform = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_dataset = datasets.MNIST(
        root="./data", train=True, download=True, transform=transform
    )
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)

    # Modelo
    config = VAEConfig(
        input_dim=784,
        hidden_dims=(512, 256),
        latent_dim=20,
        beta=1.0,
        learning_rate=1e-3,
    )

    model = VAE(config)

    # Entrenar
    losses = train_vae(model, train_loader, config, num_epochs=20, device=device)

    # Generar muestras
    model.eval()
    samples = model.sample(64, device=device)
    samples = samples.view(64, 1, 28, 28).cpu()

    # Visualizar
    from torchvision.utils import make_grid, save_image
    grid = make_grid(samples, nrow=8, padding=2)
    save_image(grid, "vae_samples.png")

    print("Muestras guardadas en vae_samples.png")

    return model


if __name__ == "__main__":
    demo_vae_mnist()
```

---

## Beta-VAE: Control del Disentanglement

```
BETA-VAE: REPRESENTACIONES DISENTANGLED
=======================================

Î²-VAE es una variante que usa Î² > 1 en el termino KL
para forzar representaciones mas "disentangled".


QUE ES DISENTANGLEMENT?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Una representacion es "disentangled" cuando cada dimension
del latent controla UN factor de variacion independiente.

Ejemplo con caras:

Entangled (VAE normal):              Disentangled (Î²-VAE):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

zâ‚: ???                              zâ‚: Pose (izq/der)
zâ‚‚: ???                              zâ‚‚: Sonrisa
zâ‚ƒ: ???                              zâ‚ƒ: Color de pelo
zâ‚„: ???                              zâ‚„: Edad

Cambiar zâ‚ afecta                    Cambiar zâ‚ SOLO
multiples atributos                  cambia la pose


EFECTO DE Î²:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Loss = Recon + Î² Â· KL

Î² = 1 (VAE estandar):
    - Balance reconstruccion/regularizacion
    - Latents pueden estar entangled

Î² > 1 (Î²-VAE):
    - Mas peso en KL
    - Fuerza latents hacia N(0,I)
    - Cada dimension mas independiente
    - Pero: peor reconstruccion


VISUALIZACION DEL TRADE-OFF:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Calidad                                  Disentanglement
    Recon â†‘                                        â†‘
        â”‚                                          â”‚
        â”‚    â—                                     â”‚              â—
        â”‚      â—                                   â”‚            â—
        â”‚        â—                                 â”‚          â—
        â”‚          â—                               â”‚        â—
        â”‚            â—                             â”‚      â—
        â”‚              â—                           â”‚    â—
        â”‚                â—                         â”‚  â—
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Î²           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Î²
              Î² optimo â‰ˆ 4-10                     Î² alto = mejor


TRAVERSAL DE LATENTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Para verificar disentanglement, variamos una dimension:

zâ‚ (pose):
    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
    â”‚ â—€ â”‚ â— â”‚ â—‹ â”‚ â–· â”‚ â–¶ â”‚  <- Solo rota la cara
    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
    -2   -1   0   +1  +2

zâ‚‚ (sonrisa):
    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
    â”‚ ğŸ˜ â”‚ ğŸ™‚ â”‚ ğŸ˜Š â”‚ ğŸ˜„ â”‚ ğŸ˜ â”‚  <- Solo cambia sonrisa
    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
    -2   -1   0   +1  +2
```

### Implementacion Beta-VAE

```python
"""
Beta-VAE para representaciones disentangled.
"""

import torch
import torch.nn as nn
from torch import Tensor
from typing import Tuple


class BetaVAE(nn.Module):
    """
    Beta-VAE: VAE con beta > 1 para disentanglement.

    Loss = Recon + Î² Â· KL
    """

    def __init__(
        self,
        input_channels: int = 1,
        latent_dim: int = 10,
        beta: float = 4.0,
        hidden_dims: Tuple[int, ...] = (32, 64, 128, 256),
    ):
        super().__init__()

        self.latent_dim = latent_dim
        self.beta = beta

        # Encoder CNN
        encoder_layers = []
        in_ch = input_channels

        for h_dim in hidden_dims:
            encoder_layers.extend([
                nn.Conv2d(in_ch, h_dim, kernel_size=4, stride=2, padding=1),
                nn.BatchNorm2d(h_dim),
                nn.LeakyReLU(0.2),
            ])
            in_ch = h_dim

        self.encoder = nn.Sequential(*encoder_layers)

        # Flatten size (asumiendo input 64x64)
        self.flatten_size = hidden_dims[-1] * 4 * 4

        # Latent projections
        self.fc_mu = nn.Linear(self.flatten_size, latent_dim)
        self.fc_logvar = nn.Linear(self.flatten_size, latent_dim)

        # Decoder
        self.fc_decode = nn.Linear(latent_dim, self.flatten_size)

        decoder_layers = []
        hidden_dims_rev = list(reversed(hidden_dims))

        for i in range(len(hidden_dims_rev) - 1):
            decoder_layers.extend([
                nn.ConvTranspose2d(
                    hidden_dims_rev[i], hidden_dims_rev[i+1],
                    kernel_size=4, stride=2, padding=1
                ),
                nn.BatchNorm2d(hidden_dims_rev[i+1]),
                nn.LeakyReLU(0.2),
            ])

        decoder_layers.extend([
            nn.ConvTranspose2d(
                hidden_dims_rev[-1], input_channels,
                kernel_size=4, stride=2, padding=1
            ),
            nn.Sigmoid(),
        ])

        self.decoder = nn.Sequential(*decoder_layers)

    def encode(self, x: Tensor) -> Tuple[Tensor, Tensor]:
        """Encode input a distribucion latente."""
        h = self.encoder(x)
        h = h.view(h.size(0), -1)
        return self.fc_mu(h), self.fc_logvar(h)

    def decode(self, z: Tensor) -> Tensor:
        """Decode latent a imagen."""
        h = self.fc_decode(z)
        h = h.view(h.size(0), -1, 4, 4)
        return self.decoder(h)

    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:
        """Reparameterization trick."""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + std * eps

    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

    def loss_function(
        self,
        x: Tensor,
        recon: Tensor,
        mu: Tensor,
        logvar: Tensor,
    ) -> Tuple[Tensor, Tensor, Tensor]:
        """
        Beta-VAE loss.
        """
        # Reconstruction (MSE para imagenes continuas)
        recon_loss = nn.functional.mse_loss(recon, x, reduction='sum')

        # KL divergence
        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

        # Total con beta
        total = recon_loss + self.beta * kl_loss

        return total, recon_loss, kl_loss

    @torch.no_grad()
    def traverse_latent(
        self,
        x: Tensor,
        dim: int,
        range_val: float = 3.0,
        num_steps: int = 11,
    ) -> Tensor:
        """
        Genera traversal de una dimension latente.

        Args:
            x: Imagen de referencia [1, C, H, W]
            dim: Dimension a variar
            range_val: Rango de variacion [-range, +range]
            num_steps: Numero de pasos

        Returns:
            Traversal images [num_steps, C, H, W]
        """
        mu, _ = self.encode(x)
        mu = mu.repeat(num_steps, 1)

        # Variar dimension especifica
        values = torch.linspace(-range_val, range_val, num_steps, device=x.device)
        mu[:, dim] = values

        return self.decode(mu)
```

---

## VQ-VAE: Vector Quantized VAE

```
VQ-VAE: VARIATIONAL AUTOENCODER CUANTIZADO
==========================================

VQ-VAE reemplaza el espacio latente continuo por uno DISCRETO,
usando un codebook de vectores aprendidos.


DIFERENCIA CON VAE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

VAE:
    x â†’ Encoder â†’ z âˆˆ â„áµˆ (continuo) â†’ Decoder â†’ xÌ‚
                   â”‚
                   â””â”€ z ~ N(Î¼, ÏƒÂ²)

VQ-VAE:
    x â†’ Encoder â†’ z_e â†’ Quantize â†’ z_q âˆˆ Codebook â†’ Decoder â†’ xÌ‚
                          â”‚
                          â””â”€ z_q = argmin ||z_e - e_k||
                                    k


CODEBOOK (Diccionario de embeddings):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Codebook E = {eâ‚, eâ‚‚, ..., e_K}

    Cada e_k âˆˆ â„áµˆ es un "embedding" aprendido.

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Codebook (K vectores)                  â”‚
    â”‚                                         â”‚
    â”‚  eâ‚: [0.2, -0.1, 0.5, ...]            â”‚
    â”‚  eâ‚‚: [0.8, 0.3, -0.2, ...]            â”‚
    â”‚  eâ‚ƒ: [-0.1, 0.7, 0.1, ...]            â”‚
    â”‚  ...                                   â”‚
    â”‚  e_K: [0.4, -0.5, 0.3, ...]           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


PROCESO DE CUANTIZACION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Encoder produce z_e (continuo)
2. Para cada vector en z_e, encontrar el embedding mas cercano:

       k* = argmin ||z_e - e_k||Â²
             k

3. Reemplazar z_e por e_{k*}:

       z_q = e_{k*}


Visualizacion 2D:

    â—  â—  â—  â—  â—  â—   <- Codebook embeddings (fijos durante forward)
    â”‚
    â”‚    Ã—             <- z_e (output del encoder)
    â”‚    â”‚
    â”‚    â””â”€â”€â”€â–¶ â—       <- z_q (embedding mas cercano)


LOSS FUNCTION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

L = L_recon + L_codebook + Î² Â· L_commit

Donde:

1. L_recon = ||x - xÌ‚||Â²
   (Reconstruccion normal)

2. L_codebook = ||sg[z_e] - e||Â²
   (Mover embeddings hacia encoder outputs)
   sg[] = stop gradient

3. L_commit = ||z_e - sg[e]||Â²
   (Mover encoder outputs hacia embeddings)
   "Commitment loss"


STRAIGHT-THROUGH ESTIMATOR:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Como la cuantizacion no es diferenciable, usamos:

    Forward:  z_q = quantize(z_e)
    Backward: âˆ‚L/âˆ‚z_e = âˆ‚L/âˆ‚z_q  (copiar gradiente)

En codigo:
    z_q = z_e + (quantize(z_e) - z_e).detach()

    Durante forward: z_q = quantize(z_e)
    Durante backward: gradientes fluyen a z_e directamente
```

### Implementacion VQ-VAE

```python
"""
VQ-VAE: Vector Quantized VAE
Implementacion con PyTorch.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
from typing import Tuple


class VectorQuantizer(nn.Module):
    """
    Vector Quantization layer.
    Mapea vectores continuos a embeddings discretos del codebook.
    """

    def __init__(
        self,
        num_embeddings: int,  # K: tamano del codebook
        embedding_dim: int,   # D: dimension de cada embedding
        commitment_cost: float = 0.25,
    ):
        super().__init__()

        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.commitment_cost = commitment_cost

        # Codebook: [K, D]
        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)

        # Inicializacion uniforme
        self.embeddings.weight.data.uniform_(
            -1.0 / num_embeddings,
            1.0 / num_embeddings
        )

    def forward(self, z: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
        """
        Cuantiza los inputs.

        Args:
            z: Encoder output [B, D, H, W]

        Returns:
            z_q: Cuantizado [B, D, H, W]
            loss: Codebook + commitment loss
            indices: Indices del codebook [B, H, W]
        """
        # Reshape: [B, D, H, W] -> [B*H*W, D]
        B, D, H, W = z.shape
        z_flat = z.permute(0, 2, 3, 1).contiguous().view(-1, D)

        # Calcular distancias al codebook: ||z - e||Â²
        # = ||z||Â² + ||e||Â² - 2*zÂ·e
        distances = (
            torch.sum(z_flat ** 2, dim=1, keepdim=True)
            + torch.sum(self.embeddings.weight ** 2, dim=1)
            - 2 * torch.matmul(z_flat, self.embeddings.weight.t())
        )

        # Encontrar embedding mas cercano
        indices = torch.argmin(distances, dim=1)

        # Obtener embeddings cuantizados
        z_q_flat = self.embeddings(indices)

        # Reshape back: [B*H*W, D] -> [B, D, H, W]
        z_q = z_q_flat.view(B, H, W, D).permute(0, 3, 1, 2).contiguous()
        indices = indices.view(B, H, W)

        # Losses
        # Codebook loss: mover embeddings hacia z_e
        codebook_loss = F.mse_loss(z_q, z.detach())

        # Commitment loss: mover z_e hacia embeddings
        commitment_loss = F.mse_loss(z, z_q.detach())

        loss = codebook_loss + self.commitment_cost * commitment_loss

        # Straight-through estimator
        z_q = z + (z_q - z).detach()

        return z_q, loss, indices

    def get_codebook_entry(self, indices: Tensor) -> Tensor:
        """Obtiene embeddings dado indices."""
        return self.embeddings(indices)


class VQVAEEncoder(nn.Module):
    """Encoder para VQ-VAE."""

    def __init__(
        self,
        in_channels: int = 3,
        hidden_dims: Tuple[int, ...] = (32, 64, 128),
        embedding_dim: int = 64,
    ):
        super().__init__()

        layers = []
        prev_ch = in_channels

        for h_dim in hidden_dims:
            layers.extend([
                nn.Conv2d(prev_ch, h_dim, kernel_size=4, stride=2, padding=1),
                nn.BatchNorm2d(h_dim),
                nn.ReLU(inplace=True),
            ])
            prev_ch = h_dim

        # Projection to embedding dim
        layers.append(
            nn.Conv2d(hidden_dims[-1], embedding_dim, kernel_size=1)
        )

        self.encoder = nn.Sequential(*layers)

    def forward(self, x: Tensor) -> Tensor:
        return self.encoder(x)


class VQVAEDecoder(nn.Module):
    """Decoder para VQ-VAE."""

    def __init__(
        self,
        out_channels: int = 3,
        hidden_dims: Tuple[int, ...] = (128, 64, 32),
        embedding_dim: int = 64,
    ):
        super().__init__()

        layers = [
            nn.Conv2d(embedding_dim, hidden_dims[0], kernel_size=1),
            nn.ReLU(inplace=True),
        ]

        for i in range(len(hidden_dims) - 1):
            layers.extend([
                nn.ConvTranspose2d(
                    hidden_dims[i], hidden_dims[i+1],
                    kernel_size=4, stride=2, padding=1
                ),
                nn.BatchNorm2d(hidden_dims[i+1]),
                nn.ReLU(inplace=True),
            ])

        layers.extend([
            nn.ConvTranspose2d(
                hidden_dims[-1], out_channels,
                kernel_size=4, stride=2, padding=1
            ),
            nn.Tanh(),  # Output en [-1, 1]
        ])

        self.decoder = nn.Sequential(*layers)

    def forward(self, z_q: Tensor) -> Tensor:
        return self.decoder(z_q)


class VQVAE(nn.Module):
    """
    VQ-VAE completo.
    """

    def __init__(
        self,
        in_channels: int = 3,
        embedding_dim: int = 64,
        num_embeddings: int = 512,
        hidden_dims: Tuple[int, ...] = (32, 64, 128),
        commitment_cost: float = 0.25,
    ):
        super().__init__()

        self.encoder = VQVAEEncoder(
            in_channels, hidden_dims, embedding_dim
        )

        self.quantizer = VectorQuantizer(
            num_embeddings, embedding_dim, commitment_cost
        )

        self.decoder = VQVAEDecoder(
            in_channels, tuple(reversed(hidden_dims)), embedding_dim
        )

    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
        """
        Forward pass.

        Returns:
            x_recon: Reconstruccion
            vq_loss: Loss de cuantizacion
            indices: Indices del codebook
        """
        z_e = self.encoder(x)
        z_q, vq_loss, indices = self.quantizer(z_e)
        x_recon = self.decoder(z_q)

        return x_recon, vq_loss, indices

    def loss_function(
        self,
        x: Tensor,
        x_recon: Tensor,
        vq_loss: Tensor,
    ) -> Tensor:
        """Calcula loss total."""
        recon_loss = F.mse_loss(x_recon, x)
        return recon_loss + vq_loss

    @torch.no_grad()
    def encode(self, x: Tensor) -> Tuple[Tensor, Tensor]:
        """Encode a indices discretos."""
        z_e = self.encoder(x)
        _, _, indices = self.quantizer(z_e)
        return indices

    @torch.no_grad()
    def decode_from_indices(self, indices: Tensor) -> Tensor:
        """Decode desde indices del codebook."""
        B, H, W = indices.shape
        z_q = self.quantizer.get_codebook_entry(indices.view(-1))
        z_q = z_q.view(B, H, W, -1).permute(0, 3, 1, 2)
        return self.decoder(z_q)


# =============================================================================
# VQ-VAE-2: VERSION JERARQUICA
# =============================================================================

class VQVAE2(nn.Module):
    """
    VQ-VAE-2: Version jerarquica con multiples niveles de cuantizacion.

    Estructura:
        - Bottom level: Alta resolucion, detalles finos
        - Top level: Baja resolucion, estructura global
    """

    def __init__(
        self,
        in_channels: int = 3,
        hidden_dims: Tuple[int, ...] = (64, 128, 256),
        num_embeddings_top: int = 512,
        num_embeddings_bottom: int = 512,
        embedding_dim: int = 64,
    ):
        super().__init__()

        # Bottom encoder (alta resolucion)
        self.encoder_bottom = nn.Sequential(
            nn.Conv2d(in_channels, hidden_dims[0], 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(hidden_dims[0], hidden_dims[1], 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(hidden_dims[1], embedding_dim, 3, 1, 1),
        )

        # Top encoder (baja resolucion)
        self.encoder_top = nn.Sequential(
            nn.Conv2d(embedding_dim, hidden_dims[2], 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(hidden_dims[2], embedding_dim, 3, 1, 1),
        )

        # Quantizers
        self.quantizer_top = VectorQuantizer(num_embeddings_top, embedding_dim)
        self.quantizer_bottom = VectorQuantizer(num_embeddings_bottom, embedding_dim)

        # Decoder top -> bottom
        self.decoder_top = nn.Sequential(
            nn.Conv2d(embedding_dim, hidden_dims[2], 3, 1, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(hidden_dims[2], embedding_dim, 4, 2, 1),
        )

        # Decoder bottom
        self.decoder_bottom = nn.Sequential(
            nn.Conv2d(embedding_dim * 2, hidden_dims[1], 3, 1, 1),  # Concat top + bottom
            nn.ReLU(),
            nn.ConvTranspose2d(hidden_dims[1], hidden_dims[0], 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(hidden_dims[0], in_channels, 4, 2, 1),
            nn.Tanh(),
        )

    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
        # Encode bottom
        z_e_bottom = self.encoder_bottom(x)

        # Encode top
        z_e_top = self.encoder_top(z_e_bottom)

        # Quantize top
        z_q_top, loss_top, indices_top = self.quantizer_top(z_e_top)

        # Decode top
        z_top_decoded = self.decoder_top(z_q_top)

        # Quantize bottom (condicionado en top)
        z_q_bottom, loss_bottom, indices_bottom = self.quantizer_bottom(z_e_bottom)

        # Decode (concatenar top y bottom)
        z_combined = torch.cat([z_top_decoded, z_q_bottom], dim=1)
        x_recon = self.decoder_bottom(z_combined)

        total_vq_loss = loss_top + loss_bottom

        return x_recon, total_vq_loss, indices_top, indices_bottom
```

---

## Aplicaciones de VAEs

```
APLICACIONES DE VAEs
====================

1. GENERACION DE DATOS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   z ~ N(0, I) â”€â”€â–¶ Decoder â”€â”€â–¶ Nueva muestra

   Aplicaciones:
   - Generacion de imagenes
   - Generacion de moleculas (drug discovery)
   - Sintesis de audio


2. COMPRESION APRENDIDA
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Original              Comprimido           Reconstruido
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”
   â”‚       â”‚  Encode    â”‚indicesâ”‚   Decode   â”‚       â”‚
   â”‚ 256KB â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚  2KB  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚ ~256KBâ”‚
   â”‚       â”‚            â”‚       â”‚            â”‚       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”˜

   VQ-VAE es especialmente util (indices discretos)


3. DETECCION DE ANOMALIAS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   VAE entrenado en datos normales.
   Anomalias tienen alta loss de reconstruccion.

   Normal:    x â”€â”€â–¶ VAE â”€â”€â–¶ xÌ‚    ||x - xÌ‚|| = bajo
   Anomalia:  x â”€â”€â–¶ VAE â”€â”€â–¶ xÌ‚    ||x - xÌ‚|| = ALTO

   Aplicaciones:
   - Deteccion de fraude
   - Monitoreo de sistemas
   - Control de calidad


4. INTERPOLACION EN ESPACIO LATENTE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   xâ‚         Interpolacion         xâ‚‚
   â”Œâ”€â”    â”Œâ”€â”  â”Œâ”€â”  â”Œâ”€â”  â”Œâ”€â”    â”Œâ”€â”
   â”‚Aâ”‚    â”‚ â”‚  â”‚ â”‚  â”‚ â”‚  â”‚ â”‚    â”‚Bâ”‚
   â””â”€â”˜    â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜    â””â”€â”˜
    â”‚      â”‚    â”‚    â”‚    â”‚      â”‚
    â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
                 â–²
           Transicion suave


5. EDICION SEMANTICA
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Encontrar direcciones en el espacio latente:

   z_original + Î± Â· direction_sonrisa = z_sonriente

   Direcciones aprendidas o encontradas:
   - Edad
   - Emocion
   - Iluminacion
   - Estilo


6. DATOS FALTANTES / INPAINTING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Imagen              Mascara            Completada
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”
   â”‚  ğŸ–¼ï¸   â”‚    +     â”‚ â–ˆâ–ˆâ–ˆâ–ˆ  â”‚    =     â”‚  ğŸ–¼ï¸   â”‚
   â”‚  ???  â”‚          â”‚       â”‚          â”‚  OK!  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”˜

   VAE puede "adivinar" partes faltantes
```

### Ejemplo: Deteccion de Anomalias

```python
"""
Deteccion de anomalias con VAE.
"""

import torch
import numpy as np
from typing import Tuple
from torch import Tensor
from torch.utils.data import DataLoader


class VAEAnomalyDetector:
    """
    Detector de anomalias basado en VAE.

    Una muestra es anomala si su loss de reconstruccion
    excede un umbral aprendido de datos normales.
    """

    def __init__(
        self,
        vae: torch.nn.Module,
        device: str = "cuda",
    ):
        self.vae = vae.to(device)
        self.vae.eval()
        self.device = device

        self.threshold: float = 0.0
        self.mean_loss: float = 0.0
        self.std_loss: float = 1.0

    @torch.no_grad()
    def compute_reconstruction_loss(self, x: Tensor) -> Tensor:
        """Calcula loss de reconstruccion por muestra."""
        x = x.to(self.device)

        if x.dim() == 3:
            x = x.unsqueeze(0)

        x_flat = x.view(x.size(0), -1)
        x_recon, mu, logvar = self.vae(x_flat)

        # Loss por muestra (no promediado)
        loss = torch.sum((x_flat - x_recon) ** 2, dim=1)

        return loss

    def fit_threshold(
        self,
        normal_loader: DataLoader,
        percentile: float = 95.0,
    ):
        """
        Ajusta el umbral usando datos normales.

        Args:
            normal_loader: DataLoader con datos normales
            percentile: Percentil para el umbral
        """
        all_losses = []

        for batch in normal_loader:
            if isinstance(batch, (list, tuple)):
                x = batch[0]
            else:
                x = batch

            losses = self.compute_reconstruction_loss(x)
            all_losses.append(losses.cpu())

        all_losses = torch.cat(all_losses).numpy()

        self.mean_loss = np.mean(all_losses)
        self.std_loss = np.std(all_losses)
        self.threshold = np.percentile(all_losses, percentile)

        print(f"Threshold ajustado: {self.threshold:.4f}")
        print(f"Mean loss: {self.mean_loss:.4f}, Std: {self.std_loss:.4f}")

    @torch.no_grad()
    def detect(
        self,
        x: Tensor,
        return_scores: bool = False,
    ) -> Tuple[Tensor, Tensor]:
        """
        Detecta anomalias.

        Args:
            x: Datos a evaluar
            return_scores: Si retornar scores normalizados

        Returns:
            is_anomaly: Boolean tensor
            scores: Anomaly scores (si return_scores=True)
        """
        losses = self.compute_reconstruction_loss(x)

        # Normalizar scores
        scores = (losses - self.mean_loss) / self.std_loss

        # Detectar anomalias
        is_anomaly = losses > self.threshold

        if return_scores:
            return is_anomaly, scores.cpu()

        return is_anomaly


def demo_anomaly_detection():
    """Demo de deteccion de anomalias con MNIST."""
    from torchvision import datasets, transforms

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Cargar modelo preentrenado (o entrenar uno)
    config = VAEConfig(input_dim=784, latent_dim=20)
    vae = VAE(config)
    # vae.load_state_dict(torch.load("vae_mnist.pt"))

    # Crear detector
    detector = VAEAnomalyDetector(vae, device)

    # Dataset: digitos 0-4 son "normales", 5-9 son "anomalias"
    transform = transforms.ToTensor()

    full_dataset = datasets.MNIST(root="./data", train=False, download=True, transform=transform)

    # Separar normales y anomalias
    normal_indices = [i for i, (_, y) in enumerate(full_dataset) if y < 5]
    anomaly_indices = [i for i, (_, y) in enumerate(full_dataset) if y >= 5]

    normal_subset = torch.utils.data.Subset(full_dataset, normal_indices[:1000])
    anomaly_subset = torch.utils.data.Subset(full_dataset, anomaly_indices[:1000])

    normal_loader = DataLoader(normal_subset, batch_size=64)

    # Ajustar umbral
    detector.fit_threshold(normal_loader, percentile=95)

    # Evaluar
    test_normal = next(iter(DataLoader(normal_subset, batch_size=100)))[0]
    test_anomaly = next(iter(DataLoader(anomaly_subset, batch_size=100)))[0]

    is_anom_normal, scores_normal = detector.detect(test_normal, return_scores=True)
    is_anom_anomaly, scores_anomaly = detector.detect(test_anomaly, return_scores=True)

    print(f"\nResultados:")
    print(f"Normales detectados como anomalia: {is_anom_normal.float().mean()*100:.1f}%")
    print(f"Anomalias detectadas: {is_anom_anomaly.float().mean()*100:.1f}%")

    return detector
```

---

## Comparacion de Variantes

```
COMPARACION: VAE vs Î²-VAE vs VQ-VAE vs VQ-VAE-2
===============================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Caracteristica â”‚    VAE      â”‚   Î²-VAE     â”‚   VQ-VAE    â”‚  VQ-VAE-2   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Espacio latenteâ”‚ Continuo    â”‚ Continuo    â”‚ Discreto    â”‚ Discreto    â”‚
â”‚                â”‚ N(Î¼, ÏƒÂ²)    â”‚ N(Î¼, ÏƒÂ²)    â”‚ Codebook    â”‚ jerarquico  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Regularizacion â”‚ KL(q||p)    â”‚ Î²Â·KL(q||p)  â”‚ VQ loss     â”‚ VQ loss     â”‚
â”‚                â”‚ Î²=1         â”‚ Î²>1         â”‚ + commit    â”‚ multi-nivel â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Calidad recon  â”‚ Media       â”‚ Baja-Media  â”‚ Alta        â”‚ Muy alta    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Disentangle    â”‚ Bajo        â”‚ Alto        â”‚ N/A         â”‚ N/A         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Generacion     â”‚ z~N(0,I)    â”‚ z~N(0,I)    â”‚ Prior       â”‚ Prior       â”‚
â”‚                â”‚             â”‚             â”‚ autoregres. â”‚ autoregres. â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Compresion     â”‚ Flotantes   â”‚ Flotantes   â”‚ Indices     â”‚ Indices     â”‚
â”‚                â”‚             â”‚             â”‚ discretos   â”‚ jerarquicos â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Uso tipico     â”‚ Generativo  â”‚ Disentangle â”‚ Compresion  â”‚ Alta calidadâ”‚
â”‚                â”‚ basico      â”‚ representa. â”‚ generacion  â”‚ imagenes    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


CUANDO USAR CADA UNO:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

VAE:
    âœ“ Punto de partida simple
    âœ“ Cuando necesitas interpolacion suave
    âœ“ Deteccion de anomalias

Î²-VAE:
    âœ“ Cuando necesitas representaciones interpretables
    âœ“ Edicion semantica controlada
    âœ“ Transferencia de atributos

VQ-VAE:
    âœ“ Compresion de alta fidelidad
    âœ“ Generacion con autoregressive prior (PixelCNN)
    âœ“ Audio (mejor que VAE continuo)

VQ-VAE-2:
    âœ“ Imagenes de alta resolucion
    âœ“ Maxima calidad de reconstruccion
    âœ“ Generacion state-of-the-art (antes de diffusion)
```

---

## Resumen

```
CONCEPTOS CLAVE - VAEs AVANZADOS
================================

1. FUNDAMENTOS VAE
   - ELBO = Reconstruccion - KL
   - Reparameterization trick para gradientes
   - Espacio latente continuo y estructurado

2. Î²-VAE
   - Î² > 1 fuerza disentanglement
   - Trade-off: disentangle vs reconstruccion
   - Util para representaciones interpretables

3. VQ-VAE
   - Espacio latente DISCRETO (codebook)
   - No KL, usa commitment + codebook loss
   - Straight-through estimator para gradientes
   - Mejor calidad de reconstruccion

4. VQ-VAE-2
   - Jerarquico: top (global) + bottom (detalles)
   - Excelente para imagenes alta resolucion
   - Base para modelos generativos pre-diffusion

5. APLICACIONES
   - Generacion de datos
   - Compresion aprendida
   - Deteccion de anomalias
   - Interpolacion/edicion semantica


FLUJO DE INFORMACION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

VAE:     x â†’ Î¼,Ïƒ â†’ z=Î¼+ÏƒÎµ â†’ xÌ‚
Î²-VAE:   x â†’ Î¼,Ïƒ â†’ z=Î¼+ÏƒÎµ â†’ xÌ‚  (Î²>1 en KL)
VQ-VAE:  x â†’ z_e â†’ quantize(z_e) â†’ xÌ‚
VQ-VAE2: x â†’ z_e â†’ [z_top, z_bot] â†’ xÌ‚


EVOLUCION HISTORICA:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

2013: VAE (Kingma & Welling)
2017: Î²-VAE (Higgins et al.)
2017: VQ-VAE (van den Oord et al.)
2019: VQ-VAE-2 (Razavi et al.)
2020: Diffusion Models superan VAEs en generacion
2022: VAE sigue siendo util para:
      - Componente de Stable Diffusion (latent space)
      - Deteccion de anomalias
      - Representaciones
```

---

## Referencias

1. Kingma, D.P. & Welling, M. (2013). "Auto-Encoding Variational Bayes"
2. Higgins, I., et al. (2017). "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework"
3. van den Oord, A., et al. (2017). "Neural Discrete Representation Learning" (VQ-VAE)
4. Razavi, A., et al. (2019). "Generating Diverse High-Fidelity Images with VQ-VAE-2"
5. Burgess, C.P., et al. (2018). "Understanding disentangling in Î²-VAE"
