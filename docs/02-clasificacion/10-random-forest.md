# Random Forest

## 1. Â¿QuÃ© es Random Forest?

### Concepto: SabidurÃ­a de la Multitud

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RANDOM FOREST = Conjunto de Ãrboles de DecisiÃ³n               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Un solo Ã¡rbol puede equivocarse fÃ¡cilmente (overfitting)      â”‚
â”‚                                                                â”‚
â”‚  SOLUCIÃ“N: Crear MUCHOS Ã¡rboles y que VOTEN                    â”‚
â”‚                                                                â”‚
â”‚       ðŸŒ³         ðŸŒ³         ðŸŒ³         ðŸŒ³         ðŸŒ³            â”‚
â”‚      Ãrbol 1   Ãrbol 2   Ãrbol 3   Ãrbol 4   Ãrbol 5          â”‚
â”‚         â”‚         â”‚         â”‚         â”‚         â”‚             â”‚
â”‚         â–¼         â–¼         â–¼         â–¼         â–¼             â”‚
â”‚       SPAM      SPAM      HAM       SPAM      SPAM            â”‚
â”‚                                                                â”‚
â”‚                        VOTACIÃ“N                                â”‚
â”‚                           â”‚                                    â”‚
â”‚                           â–¼                                    â”‚
â”‚                    4 SPAM vs 1 HAM                             â”‚
â”‚                           â”‚                                    â”‚
â”‚                           â–¼                                    â”‚
â”‚                   PREDICCIÃ“N: SPAM âœ“                           â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### AnalogÃ­a: DiagnÃ³stico MÃ©dico

```
Un solo doctor puede equivocarse.
Consultar a 100 doctores especializados y tomar la opiniÃ³n mayoritaria
es mucho mÃ¡s confiable.

Random Forest hace exactamente esto con Ã¡rboles de decisiÃ³n.
```

## 2. Ensemble Learning: El Poder del Conjunto

### Tipos de Ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENSEMBLE METHODS                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚                                               â”‚
â”‚   BAGGING       â”‚  Entrenar modelos en PARALELO                 â”‚
â”‚   (Bootstrap    â”‚  Cada modelo ve datos diferentes              â”‚
â”‚    Aggregating) â”‚  VotaciÃ³n/promedio final                      â”‚
â”‚                 â”‚  Ejemplo: Random Forest                       â”‚
â”‚                 â”‚                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚                                               â”‚
â”‚   BOOSTING      â”‚  Entrenar modelos en SECUENCIA                â”‚
â”‚                 â”‚  Cada modelo corrige errores del anterior     â”‚
â”‚                 â”‚  Ejemplo: XGBoost, AdaBoost                   â”‚
â”‚                 â”‚                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚                                               â”‚
â”‚   STACKING      â”‚  Usar predicciones de modelos base            â”‚
â”‚                 â”‚  como features para un meta-modelo            â”‚
â”‚                 â”‚                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Random Forest usa BAGGING + selecciÃ³n aleatoria de features
```

## 3. CÃ³mo Funciona Random Forest

### Paso 1: Bootstrap Sampling

```
DATOS ORIGINALES (N muestras):
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID â”‚ Feature1   â”‚ Feature2 â”‚ Clase â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1 â”‚    0.5     â”‚   100    â”‚  HAM  â”‚
â”‚  2 â”‚    0.8     â”‚   200    â”‚  SPAM â”‚
â”‚  3 â”‚    0.3     â”‚    50    â”‚  HAM  â”‚
â”‚  4 â”‚    0.9     â”‚   300    â”‚  SPAM â”‚
â”‚  5 â”‚    0.2     â”‚    30    â”‚  HAM  â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

BOOTSTRAP SAMPLE (muestreo CON reemplazo):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sample Ãrbol 1  â”‚  â”‚ Sample Ãrbol 2  â”‚  â”‚ Sample Ãrbol 3  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1, 3, 3, 4, 5   â”‚  â”‚ 2, 2, 1, 4, 3   â”‚  â”‚ 5, 1, 4, 4, 2   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                    â”‚
       â–¼                    â–¼                    â–¼
   Notar que algunas muestras se REPITEN
   y otras NO aparecen (Out-of-Bag samples)
```

### Paso 2: SelecciÃ³n Aleatoria de Features

```
FEATURES DISPONIBLES: [F1, F2, F3, F4, F5, F6, F7, F8]

En CADA NODO de CADA Ã¡rbol, solo consideramos un SUBCONJUNTO:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ãrbol 1   â”‚                                        â”‚
â”‚   Nodo A    â”‚  EvalÃºa solo: [F2, F5, F7]            â”‚
â”‚   Nodo B    â”‚  EvalÃºa solo: [F1, F3, F8]            â”‚
â”‚   Nodo C    â”‚  EvalÃºa solo: [F4, F6, F7]            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Ãrbol 2   â”‚                                        â”‚
â”‚   Nodo A    â”‚  EvalÃºa solo: [F1, F4, F6]            â”‚
â”‚   Nodo B    â”‚  EvalÃºa solo: [F2, F3, F5]            â”‚
â”‚   ...       â”‚  ...                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Â¿CuÃ¡ntas features evaluar?
  â€¢ ClasificaciÃ³n: sqrt(n_features) â‰ˆ âˆš8 â‰ˆ 3
  â€¢ RegresiÃ³n: n_features / 3 â‰ˆ 8/3 â‰ˆ 3
```

### Paso 3: ConstrucciÃ³n de Ãrboles

```
         Datos Bootstrap               Datos Bootstrap
              â”‚                             â”‚
              â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ÃRBOL 1            â”‚   â”‚      ÃRBOL 2            â”‚
â”‚  (usa features F2,F5,F7 â”‚   â”‚  (usa features F1,F4,F6 â”‚
â”‚   en nodo raÃ­z)         â”‚   â”‚   en nodo raÃ­z)         â”‚
â”‚                         â”‚   â”‚                         â”‚
â”‚      [F5 > 0.3?]        â”‚   â”‚      [F1 > 100?]        â”‚
â”‚       /      \          â”‚   â”‚       /      \          â”‚
â”‚     SÃ­        No        â”‚   â”‚     SÃ­        No        â”‚
â”‚     /          \        â”‚   â”‚     /          \        â”‚
â”‚ [F2>0.5?]    SPAM       â”‚   â”‚   HAM     [F4>50?]      â”‚
â”‚   /   \                 â”‚   â”‚            /   \        â”‚
â”‚ HAM  SPAM               â”‚   â”‚         SPAM  HAM       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Cada Ã¡rbol es DIFERENTE porque:
  1. Ve diferentes datos (bootstrap)
  2. Considera diferentes features en cada split
```

### Paso 4: AgregaciÃ³n (Voting/Averaging)

```
CLASIFICACIÃ“N - VotaciÃ³n por mayorÃ­a:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ãrbol   â”‚ PredicciÃ³n â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    1     â”‚    SPAM    â”‚
â”‚    2     â”‚    HAM     â”‚
â”‚    3     â”‚    SPAM    â”‚
â”‚    4     â”‚    SPAM    â”‚
â”‚    5     â”‚    HAM     â”‚
â”‚   ...    â”‚    ...     â”‚
â”‚   100    â”‚    SPAM    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TOTAL   â”‚ 65 SPAM    â”‚
â”‚          â”‚ 35 HAM     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  FINAL   â”‚   SPAM âœ“   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

REGRESIÃ“N - Promedio:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ãrbol   â”‚ PredicciÃ³n â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    1     â”‚    85.2    â”‚
â”‚    2     â”‚    92.1    â”‚
â”‚    3     â”‚    88.7    â”‚
â”‚   ...    â”‚    ...     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PROMEDIO â”‚   88.5     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 4. Out-of-Bag (OOB) Error

### Concepto

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OUT-OF-BAG (OOB) SAMPLES                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Bootstrap sampling deja ~37% de los datos FUERA de cada Ã¡rbol â”‚
â”‚                                                                â”‚
â”‚  Datos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]                       â”‚
â”‚                                                                â”‚
â”‚  Ãrbol 1 usa: [1, 1, 3, 5, 5, 7, 8, 8, 9, 10]                 â”‚
â”‚  OOB para Ãrbol 1: [2, 4, 6]  â† Estos NO se usaron             â”‚
â”‚                                                                â”‚
â”‚  Ãrbol 2 usa: [2, 3, 3, 4, 6, 7, 7, 8, 9, 9]                  â”‚
â”‚  OOB para Ãrbol 2: [1, 5, 10]                                  â”‚
â”‚                                                                â”‚
â”‚  Para cada muestra, calculamos su predicciÃ³n SOLO              â”‚
â”‚  usando los Ã¡rboles donde NO participÃ³ en el entrenamiento     â”‚
â”‚                                                                â”‚
â”‚  OOB Error â‰ˆ Error de validaciÃ³n cruzada (GRATIS!)            â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ventaja del OOB

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MÃ©todo tradicional    â”‚  Con OOB                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                        â”‚                                     â”‚
â”‚  1. Split train/test   â”‚  1. Usar TODOS los datos            â”‚
â”‚  2. Entrenar           â”‚  2. Entrenar con bootstrap          â”‚
â”‚  3. Evaluar en test    â”‚  3. OOB error ya calculado          â”‚
â”‚                        â”‚                                     â”‚
â”‚  Pierdes 20-30% datos  â”‚  Usas 100% datos + evaluaciÃ³n       â”‚
â”‚  para test             â”‚  gratuita                           â”‚
â”‚                        â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 5. Feature Importance

### CÃ³mo se Calcula

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MÃ‰TODOS DE IMPORTANCIA DE FEATURES                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  1. MEAN DECREASE IN IMPURITY (MDI)                            â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚     Suma de reducciones de Gini/Entropy al usar esa feature    â”‚
â”‚     promediada sobre todos los Ã¡rboles                         â”‚
â”‚                                                                â”‚
â”‚     Feature "longitud_email":                                  â”‚
â”‚       Ãrbol 1: reduce Gini en 0.15                             â”‚
â”‚       Ãrbol 2: reduce Gini en 0.12                             â”‚
â”‚       ...                                                       â”‚
â”‚       Promedio: 0.13 â† Importancia                             â”‚
â”‚                                                                â”‚
â”‚  2. PERMUTATION IMPORTANCE (mÃ¡s robusto)                       â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚     a. Medir accuracy base                                     â”‚
â”‚     b. Permutar valores de una feature (desordenar)           â”‚
â”‚     c. Medir nuevo accuracy                                    â”‚
â”‚     d. Importancia = caÃ­da en accuracy                         â”‚
â”‚                                                                â”‚
â”‚     Si permutar "longitud_email" baja accuracy de 0.95 a 0.75 â”‚
â”‚     â†’ Importancia = 0.20 (muy importante!)                     â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### VisualizaciÃ³n TÃ­pica

```
Feature Importance (MDI):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

longitud_email     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  0.28
num_links          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       0.23
palabras_urgentes  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            0.18
tiene_adjunto      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              0.16
hora_envio         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      0.08
dominio_remitente  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        0.05
num_imagenes       â–ˆâ–ˆâ–ˆ                           0.02

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Las 3 features mÃ¡s importantes explican ~70% del modelo
```

## 6. HiperparÃ¡metros Principales

### Tabla de HiperparÃ¡metros

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ParÃ¡metro        â”‚  Default  â”‚  DescripciÃ³n                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ n_estimators       â”‚    100    â”‚ NÃºmero de Ã¡rboles              â”‚
â”‚                    â”‚           â”‚ MÃ¡s = mejor, pero mÃ¡s lento    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_features       â”‚  'sqrt'   â”‚ Features a evaluar por split   â”‚
â”‚                    â”‚           â”‚ 'sqrt', 'log2', int, float     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_depth          â”‚   None    â”‚ Profundidad mÃ¡xima por Ã¡rbol   â”‚
â”‚                    â”‚           â”‚ None = sin lÃ­mite              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ min_samples_split  â”‚     2     â”‚ MÃ­nimo de muestras para split  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ min_samples_leaf   â”‚     1     â”‚ MÃ­nimo de muestras en hoja     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ bootstrap          â”‚   True    â”‚ Usar bootstrap sampling        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ oob_score          â”‚  False    â”‚ Calcular OOB error             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ n_jobs             â”‚   None    â”‚ Cores para paralelizar         â”‚
â”‚                    â”‚           â”‚ -1 = todos los cores           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ random_state       â”‚   None    â”‚ Semilla para reproducibilidad  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GuÃ­a de Ajuste

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AJUSTE DE HIPERPARÃMETROS                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  OVERFITTING? (train alto, test bajo)                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚    â€¢ Reducir max_depth (ej: 10, 20)                            â”‚
â”‚    â€¢ Aumentar min_samples_split (ej: 5, 10)                    â”‚
â”‚    â€¢ Aumentar min_samples_leaf (ej: 2, 5)                      â”‚
â”‚    â€¢ Reducir max_features                                      â”‚
â”‚                                                                â”‚
â”‚  UNDERFITTING? (train y test bajos)                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚
â”‚    â€¢ Aumentar n_estimators                                     â”‚
â”‚    â€¢ Aumentar max_depth                                        â”‚
â”‚    â€¢ Reducir min_samples_split                                 â”‚
â”‚                                                                â”‚
â”‚  MUY LENTO?                                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚    â€¢ Usar n_jobs=-1 (paralelizar)                              â”‚
â”‚    â€¢ Reducir n_estimators                                      â”‚
â”‚    â€¢ Limitar max_depth                                         â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 7. ImplementaciÃ³n en Python

### CÃ³digo BÃ¡sico

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import numpy as np

# Datos de ejemplo
X = np.random.randn(1000, 10)  # 1000 muestras, 10 features
y = (X[:, 0] + X[:, 1] > 0).astype(int)  # ClasificaciÃ³n binaria

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Crear y entrenar Random Forest
rf = RandomForestClassifier(
    n_estimators=100,        # 100 Ã¡rboles
    max_depth=10,            # Limitar profundidad
    min_samples_split=5,     # MÃ­nimo para split
    max_features='sqrt',     # sqrt(n_features)
    oob_score=True,          # Calcular OOB error
    n_jobs=-1,               # Usar todos los cores
    random_state=42          # Reproducibilidad
)

rf.fit(X_train, y_train)

# EvaluaciÃ³n
print(f"Accuracy Train: {rf.score(X_train, y_train):.3f}")
print(f"Accuracy Test:  {rf.score(X_test, y_test):.3f}")
print(f"OOB Score:      {rf.oob_score_:.3f}")
```

### Feature Importance

```python
import matplotlib.pyplot as plt

# Obtener importancias
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# Visualizar
plt.figure(figsize=(10, 6))
plt.title("Feature Importance")
plt.bar(range(X.shape[1]), importances[indices])
plt.xticks(range(X.shape[1]), [f"F{i}" for i in indices])
plt.xlabel("Feature")
plt.ylabel("Importance")
plt.tight_layout()
plt.show()

# Tabla de importancias
print("\nFeature Importance Ranking:")
print("=" * 40)
for i in indices:
    print(f"Feature {i}: {importances[i]:.4f}")
```

### Tuning con GridSearchCV

```python
from sklearn.model_selection import GridSearchCV

# Definir grid de parÃ¡metros
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10],
    'max_features': ['sqrt', 'log2']
}

# Grid Search con Cross-Validation
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"Mejores parÃ¡metros: {grid_search.best_params_}")
print(f"Mejor F1 Score (CV): {grid_search.best_score_:.3f}")

# Evaluar mejor modelo
best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(X_test)
print(classification_report(y_test, y_pred))
```

## 8. Random Forest vs Ãrbol de DecisiÃ³n

### ComparaciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Aspecto        â”‚  Ãrbol de DecisiÃ³n  â”‚   Random Forest     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Overfitting         â”‚      Alto           â”‚      Bajo           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Varianza            â”‚      Alta           â”‚      Baja           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Bias                â”‚      Bajo           â”‚      Bajo           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Interpretabilidad   â”‚      Alta           â”‚      Media          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Velocidad Train     â”‚      RÃ¡pido         â”‚      Medio          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Velocidad Predict   â”‚      RÃ¡pido         â”‚      Medio          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Manejo de ruido     â”‚      Malo           â”‚      Bueno          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Estabilidad         â”‚      Baja           â”‚      Alta           â”‚
â”‚ (cambio en datos)   â”‚                     â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Trade-off Bias-Variance

```
         Error
            â”‚
            â”‚    \
            â”‚     \      Varianza (Random Forest)
            â”‚      \____________________________
            â”‚
            â”‚    \
            â”‚     \     Varianza (Ãrbol solo)
            â”‚      \
            â”‚       \
            â”‚        \
            â”‚         \
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Complejidad del modelo

Random Forest reduce VARIANZA manteniendo BIAS bajo
(promediando muchos Ã¡rboles de alta varianza)
```

## 9. Ejemplo PrÃ¡ctico: DetecciÃ³n de Intrusiones de Red

### Dataset y Preprocesamiento

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# Simular datos de trÃ¡fico de red
np.random.seed(42)
n_samples = 5000

# Features de conexiones de red
data = {
    'duracion': np.random.exponential(10, n_samples),
    'bytes_enviados': np.random.exponential(5000, n_samples),
    'bytes_recibidos': np.random.exponential(10000, n_samples),
    'paquetes_enviados': np.random.poisson(50, n_samples),
    'paquetes_recibidos': np.random.poisson(100, n_samples),
    'conexiones_fallidas': np.random.poisson(2, n_samples),
    'num_puertos_destino': np.random.poisson(3, n_samples),
    'flag_syn': np.random.binomial(1, 0.3, n_samples),
    'flag_fin': np.random.binomial(1, 0.2, n_samples),
    'hora_del_dia': np.random.randint(0, 24, n_samples),
}

df = pd.DataFrame(data)

# Crear etiquetas basadas en patrones sospechosos
df['es_ataque'] = (
    (df['conexiones_fallidas'] > 3) |  # Muchos fallos â†’ scan
    (df['num_puertos_destino'] > 5) |   # Port scanning
    ((df['bytes_enviados'] > 20000) & (df['duracion'] < 1)) |  # DDoS
    ((df['flag_syn'] == 1) & (df['flag_fin'] == 0) & (df['duracion'] < 0.1))  # SYN flood
).astype(int)

# AÃ±adir ruido
ruido = np.random.binomial(1, 0.05, n_samples)
df['es_ataque'] = (df['es_ataque'] + ruido) % 2

print(f"DistribuciÃ³n de clases:")
print(df['es_ataque'].value_counts())
print(f"\nRatio ataque: {df['es_ataque'].mean():.2%}")
```

### Entrenamiento y EvaluaciÃ³n

```python
# Preparar datos
X = df.drop('es_ataque', axis=1)
y = df['es_ataque']

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Escalar (opcional para RF, pero buena prÃ¡ctica)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Entrenar Random Forest
rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    class_weight='balanced',  # Para datos desbalanceados
    oob_score=True,
    n_jobs=-1,
    random_state=42
)

rf.fit(X_train, y_train)

# MÃ©tricas
print("=" * 60)
print("RESULTADOS DEL MODELO")
print("=" * 60)
print(f"\nOOB Score: {rf.oob_score_:.4f}")
print(f"Train Accuracy: {rf.score(X_train, y_train):.4f}")
print(f"Test Accuracy: {rf.score(X_test, y_test):.4f}")

y_pred = rf.predict(X_test)
print("\nClassification Report:")
print(classification_report(y_test, y_pred,
      target_names=['Normal', 'Ataque']))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
```

### AnÃ¡lisis de Features

```python
# Feature importance
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("\n" + "=" * 60)
print("FEATURE IMPORTANCE - INDICADORES DE ATAQUE")
print("=" * 60)
for _, row in feature_importance.iterrows():
    bar = 'â–ˆ' * int(row['importance'] * 50)
    print(f"{row['feature']:25} {bar} {row['importance']:.4f}")

# InterpretaciÃ³n
print("\n" + "=" * 60)
print("INTERPRETACIÃ“N PARA CIBERSEGURIDAD")
print("=" * 60)
top_features = feature_importance.head(3)['feature'].tolist()
print(f"""
Los principales indicadores de ataque son:
  1. {top_features[0]}: Valores anÃ³malos sugieren actividad maliciosa
  2. {top_features[1]}: PatrÃ³n comÃºn en reconocimiento de red
  3. {top_features[2]}: Indicador de tÃ©cnicas de evasiÃ³n

RecomendaciÃ³n: Configurar alertas SIEM basadas en umbrales
de estas features para detecciÃ³n temprana.
""")
```

### Output Esperado

```
============================================================
RESULTADOS DEL MODELO
============================================================

OOB Score: 0.9125
Train Accuracy: 0.9875
Test Accuracy: 0.9150

Classification Report:
              precision    recall  f1-score   support

      Normal       0.93      0.95      0.94       720
      Ataque       0.87      0.83      0.85       280

    accuracy                           0.91      1000
   macro avg       0.90      0.89      0.89      1000
weighted avg       0.91      0.91      0.91      1000

Confusion Matrix:
[[684  36]
 [ 49 231]]

============================================================
FEATURE IMPORTANCE - INDICADORES DE ATAQUE
============================================================
conexiones_fallidas       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.2145
num_puertos_destino       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.1987
bytes_enviados            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.1523
flag_syn                  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.1234
duracion                  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.0987
...
```

## 10. Ventajas y Desventajas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VENTAJAS DE RANDOM FOREST                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  âœ“ Muy robusto contra overfitting (vs Ã¡rbol individual)        â”‚
â”‚  âœ“ Maneja bien datos con ruido y outliers                      â”‚
â”‚  âœ“ No requiere mucho preprocesamiento (scaling no necesario)   â”‚
â”‚  âœ“ Funciona bien con features numÃ©ricas y categÃ³ricas          â”‚
â”‚  âœ“ Proporciona importancia de features                         â”‚
â”‚  âœ“ OOB error como estimaciÃ³n gratuita del error                â”‚
â”‚  âœ“ Paralelizable (n_jobs=-1)                                   â”‚
â”‚  âœ“ Pocos hiperparÃ¡metros crÃ­ticos                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DESVENTAJAS DE RANDOM FOREST                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  âœ— Menos interpretable que un Ã¡rbol individual                 â”‚
â”‚  âœ— Puede ser lento con muchos Ã¡rboles y datos grandes          â”‚
â”‚  âœ— Modelo grande en memoria (todos los Ã¡rboles)                â”‚
â”‚  âœ— No extrapola bien fuera del rango de entrenamiento          â”‚
â”‚  âœ— Puede tener problemas con datos muy desbalanceados          â”‚
â”‚  âœ— No captura relaciones lineales tan bien como modelos        â”‚
â”‚    lineales                                                     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 11. CuÃ¡ndo Usar Random Forest

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CASOS DE USO IDEALES                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  âœ“ ClasificaciÃ³n de malware                                    â”‚
â”‚  âœ“ DetecciÃ³n de intrusiones/anomalÃ­as                          â”‚
â”‚  âœ“ AnÃ¡lisis de fraude                                          â”‚
â”‚  âœ“ PredicciÃ³n de churn                                         â”‚
â”‚  âœ“ DiagnÃ³stico mÃ©dico                                          â”‚
â”‚  âœ“ Cuando necesitas importancia de features                    â”‚
â”‚  âœ“ Como baseline robusto antes de probar modelos complejos     â”‚
â”‚  âœ“ Datos con muchas features (alta dimensionalidad)            â”‚
â”‚                                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  EVITAR CUANDO                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  âœ— Necesitas modelo muy interpretable (usar Ã¡rbol simple)      â”‚
â”‚  âœ— Latencia crÃ­tica en producciÃ³n (muchos Ã¡rboles = lento)     â”‚
â”‚  âœ— Datos principalmente texto/secuencias (usar DL)             â”‚
â”‚  âœ— Memoria muy limitada                                        â”‚
â”‚  âœ— Relaciones estrictamente lineales (usar regresiÃ³n)          â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 12. Resumen

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RANDOM FOREST - RESUMEN                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  CONCEPTO:                                                     â”‚
â”‚    Ensemble de mÃºltiples Ã¡rboles de decisiÃ³n                   â”‚
â”‚    Cada Ã¡rbol ve datos y features diferentes                   â”‚
â”‚    PredicciÃ³n final por votaciÃ³n/promedio                      â”‚
â”‚                                                                â”‚
â”‚  COMPONENTES CLAVE:                                            â”‚
â”‚    â€¢ Bootstrap Sampling: muestreo con reemplazo                â”‚
â”‚    â€¢ Random Feature Selection: subconjunto por nodo            â”‚
â”‚    â€¢ AgregaciÃ³n: votaciÃ³n (clasificaciÃ³n) o promedio (reg)     â”‚
â”‚                                                                â”‚
â”‚  HIPERPARÃMETROS PRINCIPALES:                                  â”‚
â”‚    â€¢ n_estimators: nÃºmero de Ã¡rboles (mÃ¡s = mejor)             â”‚
â”‚    â€¢ max_depth: profundidad mÃ¡xima (controla overfitting)      â”‚
â”‚    â€¢ max_features: features por split ('sqrt' default)         â”‚
â”‚                                                                â”‚
â”‚  VENTAJAS:                                                     â”‚
â”‚    â€¢ Robusto, pocos hiperparÃ¡metros crÃ­ticos                   â”‚
â”‚    â€¢ Feature importance incluida                                â”‚
â”‚    â€¢ OOB error como validaciÃ³n gratuita                        â”‚
â”‚                                                                â”‚
â”‚  EN CIBERSEGURIDAD:                                            â”‚
â”‚    â€¢ DetecciÃ³n de intrusiones                                  â”‚
â”‚    â€¢ ClasificaciÃ³n de malware                                  â”‚
â”‚    â€¢ AnÃ¡lisis de logs                                          â”‚
â”‚    â€¢ DetecciÃ³n de phishing                                     â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Siguiente:** Support Vector Machines (SVM)
