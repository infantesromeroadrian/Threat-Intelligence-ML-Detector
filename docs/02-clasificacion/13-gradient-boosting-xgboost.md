# Gradient Boosting y XGBoost

## 1. Â¿QuÃ© es Boosting?

### Concepto: Aprender de los Errores

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BOOSTING = Entrenar modelos SECUENCIALMENTE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Cada modelo nuevo se enfoca en los ERRORES del anterior       â”‚
â”‚                                                                â”‚
â”‚  Modelo 1:  Predice â†’ Comete errores en muestras {3, 7, 12}    â”‚
â”‚       â”‚                                                        â”‚
â”‚       â–¼                                                        â”‚
â”‚  Modelo 2:  Se enfoca en {3, 7, 12} â†’ Errores en {7, 15}       â”‚
â”‚       â”‚                                                        â”‚
â”‚       â–¼                                                        â”‚
â”‚  Modelo 3:  Se enfoca en {7, 15} â†’ Menos errores               â”‚
â”‚       â”‚                                                        â”‚
â”‚       â–¼                                                        â”‚
â”‚      ...                                                       â”‚
â”‚       â”‚                                                        â”‚
â”‚       â–¼                                                        â”‚
â”‚  PredicciÃ³n Final: CombinaciÃ³n ponderada de todos              â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Bagging vs Boosting

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BAGGING (Random Forest)          BOOSTING (Gradient Boosting)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Modelos en PARALELO              Modelos en SECUENCIA          â”‚
â”‚                                                                 â”‚
â”‚    ğŸŒ³  ğŸŒ³  ğŸŒ³  ğŸŒ³                    ğŸŒ³ â†’ ğŸŒ³ â†’ ğŸŒ³ â†’ ğŸŒ³            â”‚
â”‚    â”‚   â”‚   â”‚   â”‚                    â”‚     â”‚     â”‚     â”‚         â”‚
â”‚    â–¼   â–¼   â–¼   â–¼                    â””â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”˜         â”‚
â”‚    â”€â”€â”€â”€â”¬â”€â”€â”€â”€                            â”‚     â”‚     â”‚           â”‚
â”‚        â”‚                          Cada uno corrige              â”‚
â”‚     VOTACIÃ“N                      los errores del anterior      â”‚
â”‚                                                                 â”‚
â”‚  Reduce VARIANZA                  Reduce BIAS                   â”‚
â”‚  (promediando)                    (aprendiendo errores)         â”‚
â”‚                                                                 â”‚
â”‚  MÃ¡s robusto                      Mayor accuracy potencial      â”‚
â”‚  Menos riesgo overfitting         MÃ¡s riesgo overfitting        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. Gradient Boosting: La Idea

### Descenso por Gradiente

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GRADIENT BOOSTING                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  En lugar de ponderar muestras, usamos el GRADIENTE            â”‚
â”‚  de la funciÃ³n de pÃ©rdida para guiar el aprendizaje            â”‚
â”‚                                                                â”‚
â”‚  PASO 1: Entrenar modelo inicial Fâ‚€                            â”‚
â”‚                                                                â”‚
â”‚  PASO 2: Calcular RESIDUOS (errores)                           â”‚
â”‚          ráµ¢ = yáµ¢ - Fâ‚€(xáµ¢)                                      â”‚
â”‚                                                                â”‚
â”‚  PASO 3: Entrenar nuevo modelo hâ‚ para predecir RESIDUOS       â”‚
â”‚          hâ‚ aprende a corregir los errores de Fâ‚€               â”‚
â”‚                                                                â”‚
â”‚  PASO 4: Actualizar modelo                                     â”‚
â”‚          Fâ‚(x) = Fâ‚€(x) + Î±Â·hâ‚(x)                               â”‚
â”‚          (Î± = learning rate)                                   â”‚
â”‚                                                                â”‚
â”‚  PASO 5: Repetir pasos 2-4 hasta M modelos                     â”‚
â”‚          F_M(x) = Fâ‚€(x) + Î±Â·hâ‚(x) + Î±Â·hâ‚‚(x) + ... + Î±Â·h_M(x)  â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### VisualizaciÃ³n del Proceso

```
IteraciÃ³n 1: Modelo inicial
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Datos reales:    â—     â—  â—     â—   â—
PredicciÃ³n Fâ‚€:   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Residuos:        â†‘     â†‘  â†‘     â†“   â†‘   (diferencia real - predicho)


IteraciÃ³n 2: Corregir residuos
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PredicciÃ³n Fâ‚€:   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
+ hâ‚ (correcciÃ³n):    â•±â•²  â•±â•²
= Fâ‚:            â”€â”€â”€â•±â”€â”€â•²â•±â”€â”€â•²â”€â”€â”€â”€â”€â”€   (mÃ¡s cerca de los datos)


IteraciÃ³n N: Ajuste fino
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
F_N:             â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€   (casi perfecto)

Cada iteraciÃ³n reduce el error residual
```

## 3. Algoritmos de Gradient Boosting

### EvoluciÃ³n HistÃ³rica

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EVOLUCIÃ“N DE GRADIENT BOOSTING                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  2001: Gradient Boosting Machine (GBM)                          â”‚
â”‚        â””â”€â”€ Algoritmo original de Friedman                       â”‚
â”‚                                                                 â”‚
â”‚  2014: XGBoost (eXtreme Gradient Boosting)                      â”‚
â”‚        â””â”€â”€ RegularizaciÃ³n + optimizaciones                      â”‚
â”‚        â””â”€â”€ El rey de Kaggle por aÃ±os                            â”‚
â”‚                                                                 â”‚
â”‚  2017: LightGBM (Microsoft)                                     â”‚
â”‚        â””â”€â”€ Crecimiento por hoja (leaf-wise)                     â”‚
â”‚        â””â”€â”€ Muy rÃ¡pido con datos grandes                         â”‚
â”‚                                                                 â”‚
â”‚  2017: CatBoost (Yandex)                                        â”‚
â”‚        â””â”€â”€ Manejo nativo de categÃ³ricas                         â”‚
â”‚        â””â”€â”€ Ordered boosting (reduce overfitting)                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ComparaciÃ³n de Implementaciones

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Aspecto      â”‚   XGBoost      â”‚  LightGBM   â”‚   CatBoost     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Velocidad       â”‚     â˜…â˜…â˜…â˜†      â”‚   â˜…â˜…â˜…â˜…â˜…    â”‚     â˜…â˜…â˜…â˜…       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Accuracy        â”‚    â˜…â˜…â˜…â˜…â˜…      â”‚    â˜…â˜…â˜…â˜…     â”‚    â˜…â˜…â˜…â˜…â˜…       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Memoria         â”‚     â˜…â˜…â˜…â˜†      â”‚   â˜…â˜…â˜…â˜…â˜…    â”‚     â˜…â˜…â˜…â˜…       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CategÃ³ricas     â”‚   Requiere     â”‚   BÃ¡sico    â”‚   â˜…â˜…â˜…â˜…â˜…       â”‚
â”‚                 â”‚   encoding     â”‚             â”‚   (nativo)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU Support     â”‚      â˜…â˜…â˜…â˜…     â”‚    â˜…â˜…â˜…â˜…     â”‚    â˜…â˜…â˜…â˜…â˜…       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Overfitting     â”‚   Necesita     â”‚   Necesita  â”‚   Menos        â”‚
â”‚ control         â”‚   tuning       â”‚   tuning    â”‚   propenso     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Datos pequeÃ±os  â”‚    â˜…â˜…â˜…â˜…â˜…      â”‚    â˜…â˜…â˜…â˜…     â”‚    â˜…â˜…â˜…â˜…â˜…       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Datos grandes   â”‚     â˜…â˜…â˜…â˜…      â”‚   â˜…â˜…â˜…â˜…â˜…    â”‚     â˜…â˜…â˜…â˜…       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RECOMENDACIÃ“N:
  â€¢ Empezar con XGBoost (mÃ¡s documentaciÃ³n, comunidad grande)
  â€¢ Datos muy grandes â†’ LightGBM
  â€¢ Muchas categÃ³ricas â†’ CatBoost
```

## 4. XGBoost: CaracterÃ­sticas Clave

### RegularizaciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FUNCIÃ“N OBJETIVO DE XGBOOST                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Obj = Î£ L(yáµ¢, Å·áµ¢) + Î£ Î©(fâ‚–)                                  â”‚
â”‚        â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚        Loss         RegularizaciÃ³n                             â”‚
â”‚        (error)      (penalizaciÃ³n complejidad)                 â”‚
â”‚                                                                â”‚
â”‚  Donde Î©(f) = Î³T + Â½Î»||w||Â²                                   â”‚
â”‚                                                                â”‚
â”‚    T = nÃºmero de hojas del Ã¡rbol                               â”‚
â”‚    w = pesos de las hojas                                      â”‚
â”‚    Î³ = penalizaciÃ³n por nÃºmero de hojas                        â”‚
â”‚    Î» = regularizaciÃ³n L2 sobre pesos                           â”‚
â”‚                                                                â”‚
â”‚  GBM tradicional: Solo Loss (sin regularizaciÃ³n)               â”‚
â”‚  XGBoost: Loss + RegularizaciÃ³n = Mejor generalizaciÃ³n         â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Optimizaciones de XGBoost

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OPTIMIZACIONES QUE HACEN A XGBOOST RÃPIDO                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  1. PARALELIZACIÃ“N                                             â”‚
â”‚     â€¢ ConstrucciÃ³n de Ã¡rboles en paralelo (nivel por nivel)    â”‚
â”‚     â€¢ BÃºsqueda de splits en paralelo                           â”‚
â”‚                                                                â”‚
â”‚  2. HISTOGRAMAS                                                â”‚
â”‚     â€¢ Agrupa valores continuos en bins                         â”‚
â”‚     â€¢ Reduce complejidad de O(n) a O(num_bins)                 â”‚
â”‚                                                                â”‚
â”‚  3. SPARSITY AWARENESS                                         â”‚
â”‚     â€¢ Manejo eficiente de valores faltantes                    â”‚
â”‚     â€¢ Aprende direcciÃ³n por defecto para NaN                   â”‚
â”‚                                                                â”‚
â”‚  4. CACHE OPTIMIZATION                                         â”‚
â”‚     â€¢ Acceso secuencial a memoria                              â”‚
â”‚     â€¢ Block structure para datos grandes                       â”‚
â”‚                                                                â”‚
â”‚  5. OUT-OF-CORE                                                â”‚
â”‚     â€¢ Puede procesar datos que no caben en RAM                 â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 5. HiperparÃ¡metros Principales

### Tabla de HiperparÃ¡metros

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ParÃ¡metro       â”‚  Default  â”‚  DescripciÃ³n                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    â”‚           â”‚  CONTROL DE BOOSTING           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ n_estimators       â”‚    100    â”‚  NÃºmero de Ã¡rboles             â”‚
â”‚ (num_boost_round)  â”‚           â”‚  MÃ¡s = mÃ¡s complejo            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ learning_rate      â”‚    0.3    â”‚  Peso de cada Ã¡rbol (Î·)        â”‚
â”‚ (eta)              â”‚           â”‚  Menor = mÃ¡s robusto           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    â”‚           â”‚  CONTROL DE ÃRBOLES            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_depth          â”‚     6     â”‚  Profundidad mÃ¡xima            â”‚
â”‚                    â”‚           â”‚  Menor = menos overfitting     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ min_child_weight   â”‚     1     â”‚  Suma mÃ­nima de peso en hoja   â”‚
â”‚                    â”‚           â”‚  Mayor = mÃ¡s conservador       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ subsample          â”‚    1.0    â”‚  FracciÃ³n de muestras por Ã¡rbolâ”‚
â”‚                    â”‚           â”‚  <1 = reduce overfitting       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ colsample_bytree   â”‚    1.0    â”‚  FracciÃ³n de features por Ã¡rbolâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    â”‚           â”‚  REGULARIZACIÃ“N                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ reg_alpha (Î±)      â”‚     0     â”‚  RegularizaciÃ³n L1             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ reg_lambda (Î»)     â”‚     1     â”‚  RegularizaciÃ³n L2             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gamma (Î³)          â”‚     0     â”‚  MÃ­nima reducciÃ³n de pÃ©rdida   â”‚
â”‚                    â”‚           â”‚  para hacer un split           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GuÃ­a de Tuning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ESTRATEGIA DE TUNING PARA XGBOOST                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  PASO 1: Fijar learning_rate alto (0.1-0.3) y n_estimators    â”‚
â”‚          Ajustar max_depth, min_child_weight                   â”‚
â”‚                                                                â”‚
â”‚  PASO 2: Tune subsample, colsample_bytree                      â”‚
â”‚          Valores tÃ­picos: 0.6-0.9                              â”‚
â”‚                                                                â”‚
â”‚  PASO 3: Tune regularizaciÃ³n (gamma, reg_alpha, reg_lambda)    â”‚
â”‚                                                                â”‚
â”‚  PASO 4: Reducir learning_rate (0.01-0.1)                      â”‚
â”‚          Aumentar n_estimators proporcionalmente               â”‚
â”‚          Usar early_stopping para encontrar Ã³ptimo             â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

REGLAS GENERALES:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                â”‚
â”‚  OVERFITTING? (train alto, test bajo)                          â”‚
â”‚    â€¢ Reducir max_depth (3-6)                                   â”‚
â”‚    â€¢ Reducir learning_rate                                     â”‚
â”‚    â€¢ Aumentar min_child_weight                                 â”‚
â”‚    â€¢ Reducir subsample (0.6-0.8)                               â”‚
â”‚    â€¢ Aumentar reg_alpha, reg_lambda                            â”‚
â”‚                                                                â”‚
â”‚  UNDERFITTING? (train y test bajos)                            â”‚
â”‚    â€¢ Aumentar n_estimators                                     â”‚
â”‚    â€¢ Aumentar max_depth                                        â”‚
â”‚    â€¢ Aumentar learning_rate (temporalmente)                    â”‚
â”‚    â€¢ Reducir regularizaciÃ³n                                    â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 6. ImplementaciÃ³n en Python

### XGBoost BÃ¡sico

```python
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import numpy as np

# Datos de ejemplo
np.random.seed(42)
X = np.random.randn(1000, 10)
y = (X[:, 0] + X[:, 1] * 2 + X[:, 2] ** 2 > 1).astype(int)

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Crear y entrenar XGBoost
xgb_clf = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

xgb_clf.fit(X_train, y_train)

# Evaluar
y_pred = xgb_clf.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(classification_report(y_test, y_pred))
```

### Con Early Stopping

```python
import xgboost as xgb
from sklearn.model_selection import train_test_split

# Split incluyendo validation set
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# Entrenar con early stopping
xgb_clf = xgb.XGBClassifier(
    n_estimators=1000,  # NÃºmero alto, early stopping lo detiene
    max_depth=5,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss',
    early_stopping_rounds=50  # Detener si no mejora en 50 rondas
)

xgb_clf.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    verbose=True
)

print(f"\nMejor iteraciÃ³n: {xgb_clf.best_iteration}")
print(f"Mejor score: {xgb_clf.best_score:.4f}")

# Evaluar en test
y_pred = xgb_clf.predict(X_test)
print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}")
```

### Grid Search con XGBoost

```python
from sklearn.model_selection import GridSearchCV
import xgboost as xgb

# Definir grid de parÃ¡metros
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 200],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

# Grid Search
xgb_clf = xgb.XGBClassifier(
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

grid_search = GridSearchCV(
    xgb_clf,
    param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"Mejores parÃ¡metros: {grid_search.best_params_}")
print(f"Mejor F1 (CV): {grid_search.best_score_:.4f}")

# Evaluar
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}")
```

### Feature Importance

```python
import matplotlib.pyplot as plt
import xgboost as xgb

# DespuÃ©s de entrenar...

# Importancia por gain (mÃ¡s interpretable)
importance = xgb_clf.get_booster().get_score(importance_type='gain')

# Ordenar por importancia
sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)

print("\nFeature Importance (por gain):")
print("=" * 40)
for feature, score in sorted_importance[:10]:
    print(f"{feature}: {score:.2f}")

# VisualizaciÃ³n
xgb.plot_importance(xgb_clf, importance_type='gain', max_num_features=10)
plt.title("Feature Importance (XGBoost)")
plt.tight_layout()
plt.show()
```

## 7. LightGBM

### CÃ³digo BÃ¡sico

```python
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Entrenar LightGBM
lgb_clf = lgb.LGBMClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    num_leaves=31,  # EspecÃ­fico de LightGBM
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=42
)

lgb_clf.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric='logloss',
    callbacks=[lgb.early_stopping(50)]
)

y_pred = lgb_clf.predict(X_test)
print(f"LightGBM Accuracy: {accuracy_score(y_test, y_pred):.4f}")
```

### Diferencias con XGBoost

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LIGHTGBM vs XGBOOST                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  CRECIMIENTO DEL ÃRBOL:                                        â”‚
â”‚                                                                â”‚
â”‚  XGBoost: Level-wise (nivel por nivel)                         â”‚
â”‚                                                                â”‚
â”‚       [raÃ­z]                                                   â”‚
â”‚      /      \                                                  â”‚
â”‚    [1]      [2]     â† Todo el nivel a la vez                   â”‚
â”‚   /  \     /  \                                                â”‚
â”‚  [3] [4] [5] [6]    â† Siguiente nivel completo                 â”‚
â”‚                                                                â”‚
â”‚  LightGBM: Leaf-wise (por hoja, mÃ¡s profundidad selectiva)     â”‚
â”‚                                                                â”‚
â”‚       [raÃ­z]                                                   â”‚
â”‚      /      \                                                  â”‚
â”‚    [1]      [2]                                                â”‚
â”‚   /  \                                                         â”‚
â”‚  [3] [4]            â† Solo expande hoja con mayor ganancia     â”‚
â”‚   |                                                            â”‚
â”‚  [5]                â† Sigue la mejor hoja                      â”‚
â”‚                                                                â”‚
â”‚  Ventaja: MÃ¡s rÃ¡pido, mejor para datos grandes                 â”‚
â”‚  Riesgo: MÃ¡s propenso a overfitting                            â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 8. CatBoost

### CÃ³digo BÃ¡sico

```python
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# CatBoost maneja categÃ³ricas nativamente
cat_clf = CatBoostClassifier(
    iterations=100,
    depth=6,
    learning_rate=0.1,
    random_seed=42,
    verbose=False
)

# Si tienes features categÃ³ricas:
# cat_features = [0, 3, 5]  # Ã­ndices de columnas categÃ³ricas
# cat_clf.fit(X_train, y_train, cat_features=cat_features)

cat_clf.fit(X_train, y_train, eval_set=(X_val, y_val))

y_pred = cat_clf.predict(X_test)
print(f"CatBoost Accuracy: {accuracy_score(y_test, y_pred):.4f}")
```

## 9. Ejemplo PrÃ¡ctico: DetecciÃ³n de Malware

```python
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import StandardScaler

# Simular dataset de caracterÃ­sticas de PE (Portable Executable)
np.random.seed(42)
n_samples = 5000

# Features tÃ­picas de anÃ¡lisis estÃ¡tico de malware
data = {
    # CaracterÃ­sticas del header PE
    'size_of_code': np.random.exponential(100000, n_samples),
    'size_of_initialized_data': np.random.exponential(50000, n_samples),
    'num_sections': np.random.poisson(5, n_samples),
    'entropy_code': np.random.uniform(4, 8, n_samples),
    'entropy_data': np.random.uniform(3, 8, n_samples),

    # CaracterÃ­sticas de imports
    'num_imports': np.random.poisson(50, n_samples),
    'suspicious_imports': np.random.poisson(2, n_samples),
    'num_dlls': np.random.poisson(10, n_samples),

    # CaracterÃ­sticas de strings
    'num_urls': np.random.poisson(3, n_samples),
    'num_ips': np.random.poisson(1, n_samples),
    'suspicious_strings': np.random.poisson(5, n_samples),

    # Otras
    'packed': np.random.binomial(1, 0.3, n_samples),
    'has_debug_info': np.random.binomial(1, 0.4, n_samples),
    'signed': np.random.binomial(1, 0.6, n_samples),
}

df = pd.DataFrame(data)

# Crear etiquetas basadas en patrones de malware
df['es_malware'] = (
    (df['entropy_code'] > 7.0) |  # Alta entropÃ­a = empaquetado
    (df['suspicious_imports'] > 3) |  # Muchos imports sospechosos
    ((df['packed'] == 1) & (df['signed'] == 0)) |  # Empaquetado y sin firmar
    (df['num_urls'] > 5) |  # Muchas URLs
    (df['suspicious_strings'] > 10)  # Muchos strings sospechosos
).astype(int)

# AÃ±adir ruido
ruido = np.random.binomial(1, 0.05, n_samples)
df['es_malware'] = (df['es_malware'] + ruido) % 2

print("DistribuciÃ³n de clases:")
print(df['es_malware'].value_counts())
print(f"Ratio malware: {df['es_malware'].mean():.2%}")

# Preparar datos
X = df.drop('es_malware', axis=1)
y = df['es_malware']

# Split
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print(f"\nTrain: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")

# Entrenar XGBoost con early stopping
xgb_clf = xgb.XGBClassifier(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0,
    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),  # Balance
    random_state=42,
    use_label_encoder=False,
    eval_metric='auc',
    early_stopping_rounds=50
)

xgb_clf.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    verbose=False
)

print(f"\nMejor iteraciÃ³n: {xgb_clf.best_iteration}")

# Evaluar
y_pred = xgb_clf.predict(X_test)
y_proba = xgb_clf.predict_proba(X_test)[:, 1]

print("\n" + "=" * 60)
print("DETECTOR DE MALWARE - RESULTADOS")
print("=" * 60)

print(f"\nAccuracy: {(y_pred == y_test).mean():.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred,
      target_names=['Benigno', 'Malware']))

print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# Feature Importance
print("\n" + "=" * 60)
print("FEATURE IMPORTANCE - INDICADORES DE MALWARE")
print("=" * 60)

importance = pd.DataFrame({
    'feature': X.columns,
    'importance': xgb_clf.feature_importances_
}).sort_values('importance', ascending=False)

for _, row in importance.iterrows():
    bar = 'â–ˆ' * int(row['importance'] * 50)
    print(f"{row['feature']:25} {bar} {row['importance']:.4f}")

# InterpretaciÃ³n
print("\n" + "=" * 60)
print("INTERPRETACIÃ“N PARA CIBERSEGURIDAD")
print("=" * 60)
top3 = importance.head(3)['feature'].tolist()
print(f"""
Los principales indicadores de malware identificados son:

1. {top3[0]}: Principal caracterÃ­stica discriminante
2. {top3[1]}: Segundo indicador mÃ¡s importante
3. {top3[2]}: Tercer indicador relevante

Recomendaciones:
- Priorizar anÃ¡lisis de archivos con alta {top3[0]}
- Configurar alertas automÃ¡ticas basadas en estos indicadores
- Considerar sandbox analysis para archivos sospechosos
""")

# Ejemplos de predicciÃ³n
print("\n" + "=" * 60)
print("EJEMPLOS DE CLASIFICACIÃ“N")
print("=" * 60)

# Crear ejemplos sintÃ©ticos
ejemplos = pd.DataFrame({
    'size_of_code': [50000, 200000, 100000],
    'size_of_initialized_data': [20000, 80000, 40000],
    'num_sections': [4, 8, 5],
    'entropy_code': [5.5, 7.8, 6.2],
    'entropy_data': [4.0, 7.5, 5.0],
    'num_imports': [30, 100, 50],
    'suspicious_imports': [0, 8, 2],
    'num_dlls': [8, 20, 10],
    'num_urls': [0, 10, 2],
    'num_ips': [0, 5, 1],
    'suspicious_strings': [2, 20, 5],
    'packed': [0, 1, 0],
    'has_debug_info': [1, 0, 1],
    'signed': [1, 0, 1]
})

for i, row in ejemplos.iterrows():
    pred = xgb_clf.predict(row.values.reshape(1, -1))[0]
    proba = xgb_clf.predict_proba(row.values.reshape(1, -1))[0]

    resultado = "MALWARE ğŸš¨" if pred == 1 else "BENIGNO âœ“"
    confianza = proba[pred]

    print(f"\nEjemplo {i+1}: {resultado} (confianza: {confianza:.1%})")
    print(f"  Entropy cÃ³digo: {row['entropy_code']:.1f}")
    print(f"  Imports sospechosos: {row['suspicious_imports']}")
    print(f"  Empaquetado: {'SÃ­' if row['packed'] else 'No'}")
    print(f"  Firmado: {'SÃ­' if row['signed'] else 'No'}")
```

## 10. Ventajas y Desventajas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VENTAJAS DE GRADIENT BOOSTING                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  âœ“ Estado del arte en datos tabulares                          â”‚
â”‚  âœ“ Maneja bien features de diferentes tipos                    â”‚
â”‚  âœ“ No requiere escalado de features                            â”‚
â”‚  âœ“ Proporciona feature importance                              â”‚
â”‚  âœ“ Maneja bien valores faltantes (XGBoost, LightGBM)           â”‚
â”‚  âœ“ RegularizaciÃ³n incorporada                                  â”‚
â”‚  âœ“ Early stopping para evitar overfitting                      â”‚
â”‚  âœ“ Altamente optimizado y paralelizable                        â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DESVENTAJAS DE GRADIENT BOOSTING                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  âœ— Muchos hiperparÃ¡metros que ajustar                          â”‚
â”‚  âœ— Propenso a overfitting si no se ajusta bien                 â”‚
â”‚  âœ— Entrenamiento secuencial (no tan paralelizable)             â”‚
â”‚  âœ— Puede ser lento con datasets muy grandes                    â”‚
â”‚  âœ— Menos interpretable que un Ã¡rbol simple                     â”‚
â”‚  âœ— Sensible a outliers (aunque menos que otros)                â”‚
â”‚  âœ— No extrapola bien fuera del rango de entrenamiento          â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 11. CuÃ¡ndo Usar Gradient Boosting

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CASOS DE USO IDEALES                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  âœ“ Datos tabulares estructurados                               â”‚
â”‚  âœ“ Competiciones de ML (Kaggle, etc.)                          â”‚
â”‚  âœ“ ClasificaciÃ³n de malware                                    â”‚
â”‚  âœ“ DetecciÃ³n de fraude                                         â”‚
â”‚  âœ“ Scoring de crÃ©dito                                          â”‚
â”‚  âœ“ PredicciÃ³n de churn                                         â”‚
â”‚  âœ“ Ranking y recomendaciÃ³n                                     â”‚
â”‚  âœ“ Cuando Random Forest no es suficiente                       â”‚
â”‚                                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  EVITAR CUANDO                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  âœ— Datos de imagen/texto/audio (usar Deep Learning)            â”‚
â”‚  âœ— Datos muy pequeÃ±os (<100 muestras)                          â”‚
â”‚  âœ— Necesitas interpretabilidad total                           â”‚
â”‚  âœ— Latencia de predicciÃ³n muy crÃ­tica                          â”‚
â”‚  âœ— No tienes tiempo para tuning                                â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 12. Resumen

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GRADIENT BOOSTING - RESUMEN                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  CONCEPTO:                                                     â”‚
â”‚    Entrenar Ã¡rboles SECUENCIALMENTE                            â”‚
â”‚    Cada Ã¡rbol corrige los errores del anterior                 â”‚
â”‚    PredicciÃ³n = suma ponderada de todos los Ã¡rboles            â”‚
â”‚                                                                â”‚
â”‚  IMPLEMENTACIONES:                                             â”‚
â”‚    â€¢ XGBoost: El mÃ¡s popular, buen balance                     â”‚
â”‚    â€¢ LightGBM: MÃ¡s rÃ¡pido para datos grandes                   â”‚
â”‚    â€¢ CatBoost: Mejor para features categÃ³ricas                 â”‚
â”‚                                                                â”‚
â”‚  HIPERPARÃMETROS CLAVE:                                        â”‚
â”‚    â€¢ n_estimators: nÃºmero de Ã¡rboles                           â”‚
â”‚    â€¢ learning_rate: peso de cada Ã¡rbol (0.01-0.3)              â”‚
â”‚    â€¢ max_depth: profundidad de Ã¡rboles (3-10)                  â”‚
â”‚    â€¢ subsample/colsample: regularizaciÃ³n por muestreo          â”‚
â”‚                                                                â”‚
â”‚  BEST PRACTICES:                                               â”‚
â”‚    â€¢ Usar early_stopping                                       â”‚
â”‚    â€¢ Cross-validation para tuning                              â”‚
â”‚    â€¢ Empezar con defaults, luego optimizar                     â”‚
â”‚    â€¢ Monitorear train vs validation para overfitting           â”‚
â”‚                                                                â”‚
â”‚  EN CIBERSEGURIDAD:                                            â”‚
â”‚    â€¢ DetecciÃ³n de malware (anÃ¡lisis estÃ¡tico)                  â”‚
â”‚    â€¢ DetecciÃ³n de intrusiones                                  â”‚
â”‚    â€¢ ClasificaciÃ³n de amenazas                                 â”‚
â”‚    â€¢ Scoring de riesgo                                         â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Con esto completamos los principales modelos de clasificaciÃ³n:**
1. RegresiÃ³n LogÃ­stica
2. Ãrboles de DecisiÃ³n
3. Random Forest
4. Support Vector Machines
5. Naive Bayes
6. Gradient Boosting / XGBoost
